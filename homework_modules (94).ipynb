{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3MeKai5Xj6eX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwlFrG-Tj6eY"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W8BLmtZ3j6eZ"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKRkIjT8j6eZ"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb98PPpJj6ea"
      },
      "source": [
        "**Define** a forward and backward pass procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7y2lav4dj6ea"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially.\n",
        "\n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "\n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "\n",
        "        Just write a little loop.\n",
        "        \"\"\"\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        output = input\n",
        "        for module in self.modules:\n",
        "           output = module.forward(output)\n",
        "\n",
        "        self.output = output\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "\n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)\n",
        "            gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "\n",
        "        !!!\n",
        "\n",
        "        To ech module you need to provide the input, module saw while forward pass,\n",
        "        it is used while computing gradients.\n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
        "        and NOT `input` to this Sequential module.\n",
        "\n",
        "        !!!\n",
        "\n",
        "        \"\"\"\n",
        "        # Your code goes here. ################################################\n",
        "        grad = gradOutput\n",
        "\n",
        "        for i in reversed(range(len(self.modules))):\n",
        "           module = self.modules[i]\n",
        "           if i == 0:\n",
        "              grad = module.backward(input, grad)\n",
        "           else:\n",
        "              grad = module.backward(self.modules[i - 1].output, grad)\n",
        "\n",
        "        self.gradInput = grad\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfXdYfO4j6ea"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuwvBkuNj6ea"
      },
      "source": [
        "## 1 (0.2). Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D0uoyqkpj6ea"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output =np.dot(input, self.W.T)+self.b\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput =  np.dot(gradOutput, self.W)\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradW = np.dot(gradOutput.T, input)\n",
        "        self.gradb =np.sum(gradOutput, axis=0)\n",
        "        pass\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOnHXZJj6eb"
      },
      "source": [
        "## 2. (0.2) SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "VIValI0hj6eb"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        # Your code goes here. ################################################\n",
        "        exp=np.exp(self.output)\n",
        "        sum=np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output=exp / sum\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = self.updateOutput(input)\n",
        "\n",
        "        sum= np.sum(gradOutput * self.output, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradInput = self.output * (gradOutput- sum)\n",
        "\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3DJjynj6eb"
      },
      "source": [
        "## 3. (0.2) LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xo7DRdAJj6eb"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exp=np.exp(self.output)\n",
        "        sum=np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output=self.output-np.log(sum)\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "        exp=np.exp(self.output)\n",
        "        sum=np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output=exp / sum\n",
        "        self.gradInput = gradOutput - self.output * np.sum(gradOutput, axis=1, keepdims=True)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5QdmmPj6eb"
      },
      "source": [
        "## 4. (0.3) Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # use self.EPS please\n",
        "        if self.moving_mean is None:\n",
        "            self.moving_mean = 0\n",
        "        if self.moving_variance is None:\n",
        "            self.moving_variance = 0\n",
        "\n",
        "        if self.training == True:\n",
        "          s=np.sum(input, axis=0, keepdims=True) # максимально подробно расписала как получить среднее и дисперсию\n",
        "          self.batch_mean= s/input.shape[0]\n",
        "          sq=(input-self.batch_mean)**2\n",
        "          self.batch_variance= np.sum(sq, axis=0, keepdims=True)/ input.shape[0]\n",
        "          self.output= (input-self.batch_mean)/ (self.batch_variance+self.EPS)**(1/2)\n",
        "          self.moving_mean = self.alpha * self.moving_mean + (1 - self.alpha) * self.batch_mean\n",
        "          self.moving_variance = self.alpha * self.moving_variance + (1 - self.alpha) * self.batch_variance\n",
        "        else:\n",
        "            self.output = (input - self.moving_mean)/ (self.moving_variance + self.EPS)**(1/2)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        var = np.sum(gradOutput * ((-1/2)* (input - self.batch_mean) * (self.batch_variance + self.EPS)**(-3/2)), axis=0)\n",
        "        sum=np.sum( (-2)*(input - self.batch_mean) , axis=0, keepdims=True)/input.shape[0]\n",
        "        mean_part=var * sum\n",
        "        mean=np.sum(gradOutput * (-1/ (self.batch_variance + self.EPS)**(1/2)), axis=0)+mean_part\n",
        "        self.gradInput= (gradOutput*((1/(self.batch_variance + self.EPS))**(1/2))) + (var*2*(input - self.batch_mean)/input.shape[0])+ (mean/input.shape[0])\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "metadata": {
        "id": "LJjeeBMLygDL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "8XUS3Lt-j6eb"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5zjM3jj6eb"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gackeo1cj6eb"
      },
      "source": [
        "## 5. (0.3) Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NmLQV3jXj6eb"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "          if self.training==True:\n",
        "            size=input.shape\n",
        "            self.mask = np.random.binomial(1, 1 - self.p, size=size)\n",
        "            self.output = input * self.mask/(1 - self.p)\n",
        "          else:\n",
        "            self.output = input\n",
        "          return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput=gradOutput* self.mask /(1 - self.p)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLZolHUipKW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. (2.0) Conv2d\n",
        "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
      ],
      "metadata": {
        "id": "-WHGIqJFlhz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jwZSR69UeD61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "код должен работать но я запуталась с весами\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
        "        super(Conv2d, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.bias = bias\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "         if self.padding==\"same\":\n",
        "             height=input.shape[2]\n",
        "             width = input.shape[3]\n",
        "            #  if height==width:\n",
        "            #     if ((self.stride - 1) *height  + self.kernel_size - self.stride)% 2 != 0 :\n",
        "            #        self.padding =(((self.stride - 1) *height  + self.kernel_size - self.stride) // 2)+1\n",
        "            #  else:\n",
        "             self.padding = ((self.stride - 1) *height  + self.kernel_size - self.stride) // 2\n",
        "\n",
        "         if self.padding > 0:\n",
        "            if self.padding_mode == 'zeros':\n",
        "                input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "            elif self.padding_mode == 'replicate':\n",
        "                input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='edge')\n",
        "            elif self.padding_mode == 'reflect':\n",
        "                input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='reflect')\n",
        "\n",
        "         batch_size=input.shape[0]\n",
        "         in_channels=input.shape[1]\n",
        "         height=input.shape[2]\n",
        "         width = input.shape[3]\n",
        "\n",
        "         if type(stride)==int:\n",
        "            stride1=stride2=self.stride          \n",
        "         else:\n",
        "            stride1= self.stride[0]\n",
        "            stride2= self.stride[1]\n",
        "\n",
        "         h = 1+((height - self.kernel_size) // stride1)\n",
        "         w = 1+((width - self.kernel_size ) // stride2)\n",
        "\n",
        "         self.output = np.zeros((batch_size, self.out_channels, h, w))\n",
        "\n",
        "         for bs in range(batch_size):\n",
        "              for oc in range(self.out_channels):\n",
        "                  for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        h1 = i * stride1\n",
        "                        h2 = h1 + self.kernel_size\n",
        "                        w1 = j * stride2\n",
        "                        w2 = w1 + self.kernel_size\n",
        "                        kernel = input[bs, : , h1:h2, w1:w2]\n",
        "                        self.output[bs, oc, i, j] = np.sum(kernel * self.custom_layer.weight[oc]) #код должен работать но я запуталась откуда брать веса\n",
        "                        if self.bias==True:\n",
        "                            self.output[bs, oc, i, j]=self.output[bs, oc, i, j]+ self.custom_layer.bias[oc]\n",
        "         return  self.output\n"
      ],
      "metadata": {
        "id": "ojXATqmNeHC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Conv2d(Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size,\n",
        "#                  stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
        "#         super(Conv2d, self).__init__()\n",
        "\n",
        "#         self.in_channels = in_channels\n",
        "#         self.out_channels = out_channels\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "#         self.padding = padding\n",
        "#         self.bias = bias\n",
        "#         self.padding_mode = padding_mode\n",
        "\n",
        "#     def updateOutput(self, input):\n",
        "#         # Your code goes here. ################################################\n",
        "#          if self.padding==\"same\":\n",
        "#              height=input.shape[2]\n",
        "#              width = input.shape[3]\n",
        "#             #  if height==width:\n",
        "#             #     if ((self.stride - 1) *height  + self.kernel_size - self.stride)% 2 != 0 :\n",
        "#             #        self.padding =(((self.stride - 1) *height  + self.kernel_size - self.stride) // 2)+1\n",
        "#             #  else:\n",
        "#              self.padding = ((self.stride - 1) *height  + self.kernel_size - self.stride) // 2\n",
        "\n",
        "#          if self.padding > 0:\n",
        "#             if self.padding_mode == 'zeros':\n",
        "#                 input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "#             elif self.padding_mode == 'replicate':\n",
        "#                 input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='edge')\n",
        "#             elif self.padding_mode == 'reflect':\n",
        "#                 input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='reflect')\n",
        "\n",
        "#          batch_size=input.shape[0]\n",
        "#          in_channels=input.shape[1]\n",
        "#          height=input.shape[2]\n",
        "#          width = input.shape[3]\n",
        "\n",
        "#          if type(stride)==int:\n",
        "#             stride1=stride2=self.stride\n",
        "#          else:\n",
        "#             stride1= self.stride[0]\n",
        "#             stride2= self.stride[1]\n",
        "\n",
        "#          h = 1+((height - self.kernel_size) // stride1)\n",
        "#          w = 1+((width - self.kernel_size ) // stride2)\n",
        "\n",
        "#          self.output = np.zeros((batch_size, self.out_channels, h, w))\n",
        "\n",
        "#          for bs in range(batch_size):\n",
        "#               for oc in range(self.out_channels):\n",
        "#                   for i in range(h):\n",
        "#                     for j in range(w):\n",
        "#                         h1 = i * stride1\n",
        "#                         h2 = h1 + self.kernel_size\n",
        "#                         w1 = j * stride2\n",
        "#                         w2 = w1 + self.kernel_size\n",
        "#                         kernel = input[bs, : , h1:h2, w1:w2]\n",
        "#                         self.output[bs, oc, i, j] = np.sum(kernel * self.custom_layer.weight[oc]) #код должен работать но я запуталась откуда брать веса\n",
        "#                         if self.bias==True:\n",
        "#                             self.output[bs, oc, i, j]=self.output[bs, oc, i, j]+ self.custom_layer.bias[oc]\n",
        "#          return  self.output\n",
        "\n",
        "    # def updateGradInput(self, input, gradOutput):\n",
        "    #     # Your code goes here. ################################################\n",
        "    #     return self.gradInput\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     return \"Conv2d\"\n",
        "\n",
        "# hyperparams = [\n",
        "#     # {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n",
        "#     # 'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n",
        "#     # {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 28, 'width': 28,\n",
        "#     # 'kernel_size': 5, 'stride': 2, 'padding': 2, 'bias': False, 'padding_mode': 'replicate'},\n",
        "#     # {'batch_size': 16, 'in_channels': 3, 'out_channels': 3, 'height': 64, 'width': 64,\n",
        "#     # 'kernel_size': 3, 'stride': 1, 'padding': 'same', 'bias': True, 'padding_mode': 'reflect'},\n",
        "#     {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10,\n",
        "#     'kernel_size': 2, 'stride': (1,2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n",
        "# ]\n",
        "\n",
        "\n",
        "# for _ in range(1):\n",
        "#    for params in hyperparams:\n",
        "#     batch_size = params['batch_size']\n",
        "#     print(batch_size)\n",
        "#     in_channels = params['in_channels']\n",
        "#     out_channels = params['out_channels']\n",
        "#     height = params['height']\n",
        "#     print(height)\n",
        "#     width = params['width']\n",
        "#     print(width)\n",
        "#     kernel_size = params['kernel_size']\n",
        "#     stride = params['stride']\n",
        "#     padding = params['padding']\n",
        "#     bias = params['bias']\n",
        "#     padding_mode = params['padding_mode']\n",
        "#     custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n",
        "#                                 stride=stride, padding=padding, bias=bias,\n",
        "#                                 padding_mode=padding_mode)\n",
        "#     custom_layer.train()\n",
        "\n",
        "#     torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "#                                   stride=stride, padding=padding, bias=bias,\n",
        "#                                   padding_mode=padding_mode)\n",
        "#     print(torch_layer)\n",
        "\n",
        "#     custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n",
        "#     print(custom_layer.weight)\n",
        "#     if bias:\n",
        "#         custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n",
        "\n",
        "#     layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
        "#     input_var = torch.tensor(layer_input, requires_grad=True)\n",
        "#     print(input_var)\n",
        "\n",
        "#     custom_output = custom_layer.updateOutput(layer_input)\n",
        "#     print(custom_output)\n",
        "#     torch_output = torch_layer(input_var)\n",
        "#     print(torch_output)\n",
        "#     assert(\n",
        "#         np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
        "\n",
        "#           # next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
        "#           # custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
        "#           # torch_output.backward(torch.tensor(next_layer_grad))\n",
        "#           # torch_grad = input_var.grad.detach().numpy()\n",
        "#           # self.assertTrue(\n",
        "#           #     np.allclose(torch_grad, custom_grad, atol=1e-5))"
      ],
      "metadata": {
        "id": "8pM3nhgknjwo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html). Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
      ],
      "metadata": {
        "id": "updUVZE9qixP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride, padding):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "\n",
        "        if self.padding > 0:\n",
        "           input = np.pad(array=input,\n",
        "                          pad_width=((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
        "                          mode='constant',\n",
        "                          constant_values=-np.inf )\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "\n",
        "        if type(stride)==int:\n",
        "            stride1=stride2=self.stride\n",
        "        else:\n",
        "            stride1= self.stride[0]\n",
        "            stride2= self.stride[1]\n",
        "\n",
        "        h = 1 + (height - self.kernel_size) // self.stride\n",
        "        w = 1 + (width - self.kernel_size) // self.stride\n",
        "        self.output = np.zeros((batch_size, channels, h, w))\n",
        "        self.ind = np.zeros((batch_size, channels, h, w, 2), dtype=int)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        hs = i * self.stride\n",
        "                        he = hs + self.kernel_size\n",
        "                        ws = j * self.stride\n",
        "                        we = ws + self.kernel_size\n",
        "                        kernel = input[bs, c, hs:he, ws:we]\n",
        "                        self.output[bs, c, i, j] = np.max(kernel)\n",
        "                        unr = np.argmax(kernel)\n",
        "                        unr_2d = (unr // kernel_size, unr % kernel_size)\n",
        "                        original_index = (hs + unr_2d[0]-self.padding, ws + unr_2d[1]-self.padding)\n",
        "                        self.ind[bs, c, i, j] = original_index\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "\n",
        "\n",
        "        h = 1 + ((height - self.kernel_size+2*self.padding) // self.stride)\n",
        "        w = 1 + ((width - self.kernel_size+2*self.padding) // self.stride)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        mx = self.ind[bs, c, i, j]\n",
        "                        self.gradInput[bs, c, mx[0], mx[1]] = self.gradInput[bs, c, mx[0], mx[1]]  + gradOutput[bs, c, i, j]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MaxPool2d\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VdTiaLXZ0wcR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride, padding):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "\n",
        "        if self.padding > 0:\n",
        "           input = np.pad(array=input,\n",
        "                          pad_width=((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
        "                          mode='constant',\n",
        "                          constant_values=-np.inf)\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "\n",
        "\n",
        "        h = 1 + (height - self.kernel_size) // self.stride\n",
        "        w = 1 + (width - self.kernel_size) // self.stride\n",
        "        self.output = np.zeros((batch_size, channels, h, w))\n",
        "        self.ind = np.zeros((batch_size, channels, h, w, 2), dtype=int)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        hs = i * self.stride\n",
        "                        he = hs + self.kernel_size\n",
        "                        ws = j * self.stride\n",
        "                        we = ws + self.kernel_size\n",
        "                        kernel = input[bs, c, hs:he, ws:we]\n",
        "                        self.output[bs, c, i, j] = np.max(kernel)\n",
        "                        unr = np.unravel_index(np.argmax(kernel), kernel.shape)\n",
        "                        self.ind[bs, c, i, j] = (unr[0] + hs, unr[1] + ws)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "\n",
        "        h = 1 + ((height - self.kernel_size) // self.stride)\n",
        "        w = 1 + ((width - self.kernel_size) // self.stride)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        mx = self.ind[bs, c, i, j]\n",
        "                        self.gradInput[bs, c, mx[0], mx[1]] = self.gradInput[bs, c, mx[0], mx[1]] + gradOutput[bs, c, i, j]\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MaxPool2d\"\n",
        "\n",
        "class AvgPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride, padding):\n",
        "\n",
        "        super(AvgPool2d, self).__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        if self.padding > 0:\n",
        "           input = np.pad(array=input,\n",
        "                          pad_width=((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),\n",
        "                          mode='constant')\n",
        "\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "\n",
        "        h = 1 + (height - self.kernel_size) // self.stride\n",
        "        w = 1 + (width- self.kernel_size) // self.stride\n",
        "        self.output = np.zeros((batch_size, channels, h, w))\n",
        "        self.ind = np.zeros((batch_size, channels, h, w, 2), dtype=int)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        hs = i * self.stride\n",
        "                        he = hs + self.kernel_size\n",
        "                        ws = j * self.stride\n",
        "                        we = ws + self.kernel_size\n",
        "                        kernel = input[bs, c, hs:he, ws:we]\n",
        "                        self.output[bs, c, i, j] = np.mean(kernel)\n",
        "                        unr = np.argmax(kernel)\n",
        "                        unr_2d = (unr // self.kernel_size, unr % self.kernel_size)\n",
        "                        original_index = (hs + unr_2d[0]-self.padding, ws + unr_2d[1]-self.padding)\n",
        "                        self.ind[bs, c, i, j] = original_index\n",
        "\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        batch_size = input.shape[0]\n",
        "        channels = input.shape[1]\n",
        "        height = input.shape[2]\n",
        "        width = input.shape[3]\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "\n",
        "        h = 1 + ((height - self.kernel_size+2*self.padding) // self.stride)\n",
        "        w = 1 + ((width - self.kernel_size+2*self.padding) // self.stride)\n",
        "\n",
        "        for bs in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h):\n",
        "                    for j in range(w):\n",
        "                        mx = self.ind[bs, c, i, j]\n",
        "                        self.gradInput[bs, c, mx[0], mx[1]] = self.gradInput[bs, c, mx[0], mx[1]] + gradOutput[bs, c, i, j]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AvgPool2d\""
      ],
      "metadata": {
        "id": "Qys58EzkqhLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**. They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
      ],
      "metadata": {
        "id": "KTN5R3CwrukV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
      ],
      "metadata": {
        "id": "cYeBQDBhtViy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Flatten(Module):\n",
        "    def __init__(self, start_dim=0, end_dim=-1):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "        self.start_dim = start_dim\n",
        "        self.end_dim = end_dim\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        if self.end_dim == -1:\n",
        "            self.end_dim = input.ndim - 1\n",
        "\n",
        "        flatten1 = input.shape[:self.start_dim]\n",
        "        flatten2 = input.shape[self.end_dim + 1:]\n",
        "        self.shape = flatten1 + (-1,) + flatten2\n",
        "        self.output = input.reshape(self.shape)\n",
        "\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = gradOutput.reshape(input.shape)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ],
      "metadata": {
        "id": "SimPEMOFqhTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o36vPHSSj6eb"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_pryRQIj6ec"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "sgm8bXjKj6ec"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB0UHGagj6ec"
      },
      "source": [
        "## 10. (0.1) Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agwfkwO0j6ec"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.where(input <= 0, input * self.slope, input)\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput=np.where(input <= 0, gradOutput * self.slope, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LeakyReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-STyecvj6ec"
      },
      "source": [
        "## 11. (0.1) ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jJSzEu1mj6ec"
      },
      "outputs": [],
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.where(input <= 0, self.alpha * (np.exp(input) - 1), input)\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.where(input <= 0, gradOutput * (self.alpha * np.exp(input)), gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ELU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn3C7KTqj6ec"
      },
      "source": [
        "## 12. (0.1) SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xcDPMssrj6ec"
      },
      "outputs": [],
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.log1p(np.exp(input))\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = gradOutput * (1 / (1 + np.exp(-input)))\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. (0.2) Gelu\n",
        "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
      ],
      "metadata": {
        "id": "kw3PeZjOuo0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import erf\n",
        "class Gelu(Module):\n",
        "    def __init__(self):\n",
        "        super(Gelu, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.func= (1/2) * (1 + erf(input / np.sqrt(2)))\n",
        "        self.output = input * self.func\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        grad= (1 / (2 * np.pi)**(1/2)) * np.exp((-1/2) * input**2)\n",
        "        self.gradInput = (gradOutput * self.func) + (gradOutput * input * grad)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Gelu\""
      ],
      "metadata": {
        "id": "SdieE0Dtuo8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55p7UvPAj6ec"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NFaxZaqj6ec"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XGu45A8qj6ec"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuU26xkpj6ec"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-i3VNuHhj6ec"
      },
      "outputs": [],
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LKLWNVj6ec"
      },
      "source": [
        "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "die7KvW6j6ec"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterionUnstable, self)\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = (-1/ input.shape[0]) * np.sum(np.log(input_clamp)* target)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = (-1/input.shape[0])*target*(1/ input_clamp)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHr_JbU5j6ec"
      },
      "source": [
        "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v7N8bVP9j6ec"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self)\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = (-1/input.shape[0])*np.sum(target * input)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput=-target/input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\\\n",
        "2-я часть задания: реализация моделей на своих классах. Что должно быть:\n",
        "  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.\n",
        "  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.\n",
        "  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n",
        "  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n",
        "\n",
        "Дополнительно в оценке каждой модели будет учитываться:\n",
        "1. Наличие правильно выбранной метрики и лосс функции.\n",
        "2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.\n",
        "3. Наличие шедулера для lr.\n",
        "4. Наличие вормапа.\n",
        "5. Наличие механизма ранней остановки и сохранение лучшей модели.\n",
        "6. Свитч лося (метрики) и оптимайзера."
      ],
      "metadata": {
        "id": "TC2Bf1PP2Ios"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 часть"
      ],
      "metadata": {
        "id": "KuPHx1mDttvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, parameters, learning_rate=0.05):\n",
        "        self.parameters = parameters\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self, grad):\n",
        "        for parameter, grad in zip(self.parameters, grad):\n",
        "            parameter =parameter- (self.learning_rate * grad)"
      ],
      "metadata": {
        "id": "kaorDOXHcxH9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "    def __init__(self, parameters, learning_rate=0.05, beta1=0.9, beta2=0.999, eps=1e-6):\n",
        "        self.parameters = parameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "        self.m0 = []\n",
        "        self.v0 = []\n",
        "        for parameter in parameters:\n",
        "            self.m0.append(np.zeros_like(parameter))\n",
        "            self.v0.append(np.zeros_like(parameter))\n",
        "        self.teta0 = 0\n",
        "\n",
        "    def step(self, grads):\n",
        "        self.teta0 = self.teta0 +1\n",
        "        for i, (parameter, grad) in enumerate(zip(self.parameters, grads)):\n",
        "            self.m0[i] = self.beta1 * self.m0[i] + (1 - self.beta1) * grad\n",
        "            self.v0[i] = self.beta2 * self.v0[i] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "            m_kr = self.m0[i] / (1 - self.beta1 ** self.teta0)\n",
        "            v_kr = self.v0[i] / (1 - self.beta2 ** self.teta0)\n",
        "\n",
        "            parameter =  parameter - self.learning_rate * (m_kr / (((v_kr)**(1/2)) + self.eps))\n",
        "\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "         for parameter in self.parameters:\n",
        "             parameter.fill(0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OCZuNaLVlNiE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_targets=3, noise=0.5)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "class FCNN(Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.layers = []\n",
        "        self.layers.append(Linear(input_size, hidden_layers[0]))\n",
        "        self.layers.append(BatchNormalization())\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(Dropout(p=0.5))\n",
        "        for i in range(1, len(hidden_layers)):\n",
        "            self.layers.append(Linear(hidden_layers[i-1], hidden_layers[i]))\n",
        "            self.layers.append(BatchNormalization())\n",
        "            self.layers.append(ReLU())\n",
        "            self.layers.append(Dropout(p=0.5))\n",
        "        self.layers.append(Linear(hidden_layers[-1], output_size))\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        self.output = input\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        for layer in reversed(self.layers):\n",
        "            gradOutput = layer.backward(input, gradOutput)\n",
        "            input = layer.output\n",
        "        return gradOutput\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.1):\n",
        "    criterion = MSECriterion()\n",
        "    optimizer = Adam(model.getParameters(), learning_rate=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.training = True\n",
        "        model.getGradParameters()\n",
        "        optimizer.zeroGradParameters()\n",
        "\n",
        "        outputs = model.forward(X_train)\n",
        "        loss = criterion.forward(outputs, y_train)\n",
        "        train_losses.append(loss)\n",
        "        model.backward(X_train, criterion.backward(outputs, y_train))\n",
        "        grads = [layer.gradInput for layer in model.layers if hasattr(layer, 'gradInput')]\n",
        "        optimizer.step(grads)\n",
        "        model.training = False\n",
        "        test_outputs = model.forward(X_test)\n",
        "        test_loss = criterion.forward(test_outputs, y_test)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "    return train_losses, test_losses\n",
        "\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "model = FCNN(input_size, output_size, [64, 32])\n",
        "\n",
        "train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Train and Test Losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "MQhzOvq-YOVb",
        "outputId": "294bdb75-bc1a-4f9a-cc5b-0d736ffb9ded"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAHWCAYAAAAYdUqfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAetVJREFUeJzt3X+cjPX+//Hn/N7fv/zYtWFtyEqrQj9WoiKUU5EUx7ej6DdJztGvE5EcUYkojn5+Toc61Mnph4ONSmUrREpSp5SitYk12F+zM9f3j2tmdsfPNZbLj8f9dpvbzFzXe97Xe655zXVdr+v9nmtshmEYAgAAAAAcdXarGwAAAAAAJysSMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgDAEXPDDTeoSZMmVjcjKhdddJEuuugiq5sBADjBkZABwEnIZrPV6Pb+++9b3dRj1ujRo2u0DmsrqZs/f75Gjx5d4/IXXXSRzjjjjFpZNgDgyHFa3QAAwNH38ssvRzz/xz/+ofz8/L2mt2zZ8rCW8+yzzyoQCBxWHceqq6++Ws2aNQs/37Vrl26//Xb16tVLV199dXh6enp6rSxv/vz5evrppw8pKQMAHPtIyADgJPT//t//i3j+ySefKD8/f6/peyopKVFcXFyNl+NyuaJq3/GgdevWat26dfj51q1bdfvtt6t169YHXY8AAIQwZBEAsE+hIW8rV65Ux44dFRcXpwceeECS9J///Ec9evRQZmamPB6PmjZtqrFjx8rv90fUsedvyH788UfZbDY9/vjjmjlzppo2bSqPx6NzzjlHy5cvP2ibtm3bpr/85S/Kzc1VQkKCkpKSdNlll+mLL76IKPf+++/LZrNpzpw5GjdunBo2bKiYmBh17txZ//vf//aqN9SW2NhYnXvuufrwww+jWGP79s033+iaa65RWlqaYmJi1K5dO7355psRZXw+n8aMGaPmzZsrJiZGderUUYcOHZSfny/JXI9PP/20pMjhprXhmWeeUatWreTxeJSZmanBgweruLg4osx3332n3r17KyMjQzExMWrYsKH69u2rHTt2hMvk5+erQ4cOSklJUUJCglq0aBGOl5Dy8nI99NBDatasmTwejxo1aqR77rlH5eXlEeVqUhcAnCjoIQMA7Nfvv/+uyy67TH379tX/+3//Lzz87qWXXlJCQoKGDx+uhIQELVmyRKNGjZLX69Vjjz120Hpnz56tnTt36tZbb5XNZtPEiRN19dVX64cffjhgr9oPP/ygefPmqU+fPsrOztaWLVv097//XZ06ddLXX3+tzMzMiPKPPvqo7Ha7/vKXv2jHjh2aOHGi+vfvr08//TRc5vnnn9ett96q9u3ba9iwYfrhhx905ZVXKi0tTY0aNYpyzZnWrl2rCy64QKeccoruu+8+xcfHa86cOerZs6def/119erVS5L5e7Tx48frpptu0rnnniuv16sVK1bo888/16WXXqpbb71Vmzdv3uew0sMxevRojRkzRl26dNHtt9+u9evXa/r06Vq+fLk+/vhjuVwuVVRUqFu3biovL9edd96pjIwMbdq0SW+//baKi4uVnJystWvX6g9/+INat26thx9+WB6PR//73//08ccfh5cVCAR05ZVX6qOPPtItt9yili1b6ssvv9STTz6pb7/9VvPmzQuvs4PVBQAnFAMAcNIbPHiwsecuoVOnToYkY8aMGXuVLykp2WvarbfeasTFxRllZWXhaQMGDDCysrLCzzds2GBIMurUqWNs27YtPP0///mPIcl46623DtjOsrIyw+/3R0zbsGGD4fF4jIcffjg87b333jMkGS1btjTKy8vD06dMmWJIMr788kvDMAyjoqLCqF+/vnHWWWdFlJs5c6YhyejUqdMB21Pdb7/9ZkgyHnroofC0zp07G7m5uRHrJBAIGO3btzeaN28ennbmmWcaPXr0OGD9+/qMDqRTp05Gq1at9ju/qKjIcLvdRteuXSPW6bRp0wxJxgsvvGAYhmGsWrXKkGTMnTt3v3U9+eSThiTjt99+22+Zl19+2bDb7caHH34YMX3GjBmGJOPjjz+ucV0AcCJhyCIAYL88Ho9uvPHGvabHxsaGH+/cuVNbt27VhRdeqJKSEn3zzTcHrfe6665Tampq+PmFF14oyewBO1h77HZz1+X3+/X777+Hh7R9/vnne5W/8cYb5Xa797ucFStWqKioSLfddltEuRtuuEHJyckHfR8Hsm3bNi1ZskTXXntteB1t3bpVv//+u7p166bvvvtOmzZtkiSlpKRo7dq1+u677w5rmYfi3XffVUVFhYYNGxZep5J08803KykpSe+8844khdfDwoULVVJSss+6UlJSJJlDWfd3EZe5c+eqZcuWysnJCa+LrVu36pJLLpEkvffeezWuCwBOJCRkAID9OuWUUyISlZC1a9eqV69eSk5OVlJSkurVqxe+kEX13xXtT+PGjSOeh5Kz7du3H/B1gUBATz75pJo3by6Px6O6deuqXr16WrNmzT6Xe7Dl/PTTT5Kk5s2bR5RzuVw69dRTD/o+DuR///ufDMPQyJEjVa9evYjbQw89JEkqKiqSJD388MMqLi7WaaedptzcXI0YMUJr1qw5rOUfTOi9t2jRImK62+3WqaeeGp6fnZ2t4cOH67nnnlPdunXVrVs3Pf300xHr+7rrrtMFF1ygm266Senp6erbt6/mzJkTkVB99913Wrt27V7r4rTTTotYFzWpCwBOJPyGDACwX9V7wkKKi4vVqVMnJSUl6eGHH1bTpk0VExOjzz//XPfee2+NDpwdDsc+pxuGccDX/e1vf9PIkSM1cOBAjR07VmlpabLb7Ro2bNg+lxvtcmpDqD1/+ctf1K1bt32WCV02v2PHjvr+++/1n//8R4sWLdJzzz2nJ598UjNmzNBNN910xNt6ME888YRuuOGGcPuGDh2q8ePH65NPPlHDhg0VGxurpUuX6r333tM777yjBQsW6F//+pcuueQSLVq0SA6HQ4FAQLm5uZo0adI+lxH6vV5N6gKAEwkJGQDgkLz//vv6/fff9e9//1sdO3YMT9+wYcMRX/Zrr72miy++WM8//3zE9OLiYtWtW/eQ68vKypJk9t6Ehs5J5lUPN2zYoDPPPDPqtoZ62Fwul7p06XLQ8mlpabrxxht14403ateuXerYsaNGjx4dTshq66qKIaH3vn79+ojewIqKCm3YsGGvNufm5io3N1cPPvigli1bpgsuuEAzZszQI488Ikmy2+3q3LmzOnfurEmTJulvf/ub/vrXv+q9995Tly5d1LRpU33xxRfq3LnzQd/LweoCgBMJQxYBAIck1ENRvZepoqJCzzzzzFFZ9p69W3Pnzg3/FutQtWvXTvXq1dOMGTNUUVERnv7SSy/tden3Q1W/fn1ddNFF+vvf/65ff/11r/m//fZb+PHvv/8eMS8hIUHNmjWLuBx8fHy8JB12u0K6dOkit9utp556KmKdPv/889qxY4d69OghSfJ6vaqsrIx4bW5urux2e7h927Zt26v+s846S5LCZa699lpt2rRJzz777F5lS0tLtXv37hrXBQAnEnrIAACHpH379kpNTdWAAQM0dOhQ2Ww2vfzyy0dlGOAf/vAHPfzww7rxxhvVvn17ffnll5o1a1bUv/dyuVx65JFHdOutt+qSSy7Rddddpw0bNujFF1887N+QSdLTTz+tDh06KDc3VzfffLNOPfVUbdmyRQUFBfrll1/C/592+umn66KLLlLbtm2VlpamFStW6LXXXtOQIUPCdbVt21aSNHToUHXr1k0Oh0N9+/Y94PJ/++23cA9WddnZ2erfv7/uv/9+jRkzRt27d9eVV16p9evX65lnntE555wT/k3gkiVLNGTIEPXp00ennXaaKisr9fLLL8vhcKh3796SzN/ALV26VD169FBWVpaKior0zDPPqGHDhurQoYMk6frrr9ecOXN022236b333tMFF1wgv9+vb775RnPmzNHChQvVrl27GtUFACcUC6/wCAA4Ruzvsvf7u2z6xx9/bJx//vlGbGyskZmZadxzzz3GwoULDUnGe++9Fy63v8veP/bYY3vVqT0uGb8vZWVlxp///GejQYMGRmxsrHHBBRcYBQUFRqdOnSIuUR+67P2el2oPLf/FF1+MmP7MM88Y2dnZhsfjMdq1a2csXbp0rzoPZl+XvTcMw/j++++NP/3pT0ZGRobhcrmMU045xfjDH/5gvPbaa+EyjzzyiHHuuecaKSkpRmxsrJGTk2OMGzfOqKioCJeprKw07rzzTqNevXqGzWY76CXwQ39bsK9b586dw+WmTZtm5OTkGC6Xy0hPTzduv/12Y/v27eH5P/zwgzFw4ECjadOmRkxMjJGWlmZcfPHFxrvvvhsus3jxYuOqq64yMjMzDbfbbWRmZhr9+vUzvv3224g2VVRUGBMmTDBatWpleDweIzU11Wjbtq0xZswYY8eOHYdUFwCcKGyGcRROaQIAAAAA9sJvyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFuGPoWtJIBDQ5s2blZiYKJvNZnVzAAAAAFjEMAzt3LlTmZmZstsP3AdGQlZLNm/erEaNGlndDAAAAADHiJ9//lkNGzY8YBkSslqSmJgoyVzpSUlJFrdG8vl8WrRokbp27SqXy2V1c3CcIG4QDeIG0SJ2EA3iBtE42nHj9XrVqFGjcI5wICRktSQ0TDEpKemYScji4uKUlJTExgo1RtwgGsQNokXsIBrEDaJhVdzU5KdMXNQDAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIk6rG4Da9/1vu/TT1p3asFNaX7hTyfExivc4Fed2yOO0y2az1aieisqAdpT6qt0q5LDblRTjVFKsS0kxLiXFOuVxOmpUnz9gaNvuCv22s1xbd5m333aWq8wXUGKMU4nBehNjnEqKcSk51qUEj1Oxh9juPQUChrbuLtem7aXaXFymQm+ZYl0O1Ulwq068W3USPEqLdyspxnlIywgEDO0sq9S2kgptL6mQP2Aozu1QvNupOI9DcW6n4lwO2e37r9MwDPkDhioDVfeV/kDEtJDqTQu1MxAuF5A/IFUGzNdGvs4sa6tWh9Nul9tpl8dp3rsd5r3NCMioeuk++QOGyiv9KvcFVFbpV5kvoPJKv3aX+7WrvFI7y3zaVVapnWWV2hl8HggYSov3qG6iW3XiPaqbYK73OgluJXrM9W4YhgKGFDAM8xYwl+dx2g+4Dqu3y1vqk7fMJ29ppfyGISP4Zgyp2vsyVOk315EvuL59fkOVgYAq/YbsdptinHZ5XGbcmTeHPC67/AFDO0p98ga/E+Z9pXaU+mTIUN0E872Z9x7VTTSfe5wO+fwBlZT7tauiUiXlldpd4dfu8krtKq/UrrLgfbm53naV+7SzrFJlPr/qJHiUkRSjjOSY8H16Usxe8RoIGPIFP3+f31C5z/xsSn1+lfr8KgvdV/jldtqVGOOK+M4luJ3h9Vzm82t7SYW27/aZ9yUV2r67Qna7TaekxKphaqxOSYlTrLtm3/1DYRiGft9doU3bS/XbznI5HDbFOB2KcdkV43IEb/bgNMchxUdZcD2Evlvhm2EoELx32m2KcTkU63Io1u1QjDPyO+wPGNpZ5lNxiU/FpT4Vl1RoR6lP5ZWBiG1jcqxLyXGu8Ho1DPNzCX0Woc+j3BeQw26Tw26TM3xvl8Nhk03S77sqtMVbpi07y7TFW64ib5mKdprbzvB2LCHye5UW75bTblOFP6CKSjO+fcHHFf6A7DZbVWzvEefO4HLN0LLJZgttO8zphszvqPl9Mu8NSXabzYyj4L4m2u31wWJjR6lPv+0s12+7ys3P0h/cbga/y6HnDrvM763TLo/LXvXY6ZDjAPHisNvkdtrlcpj3Hocj/NzpiDyHbVTbWBrhbZdkBNdL6Lm/2nYm9Bn4grfKgKFEj1PJcS6lxLrldu77PHlJRaV+3VGmwh1l+nVHmbZ4y1TpN+R02My22ava6LTbFOd2mjEYa+6nk2NdSoxx7fXeAwFDFf6Ayn0Blfv9Kq3wm9vussqq7Xlwu1RRGVBSqM4YZzjGk2NdinU5tLOsco9jBvNWUl6pxBiX0uLde92qx0poGxb6DP2Bqm14KP5CYVVZWamSSmlHqU9OnxH+DKqz2YKvs5nxGXq9r9KQt6xqG171uFIlFf7w52dUqzQU48mxLqXEmTdz/bqVEmceq5T5/CqpMG+lFX6VVJj1lfn8sturPifzMzM/J5fDrvJKv0orAiqpqAxvH0oqqrYP5ZXmd7e80h+On3JfQE6HLXy8Ee92Kt7jVHzw2MPlsMkfCO5Tg9u26uvT6bDLYd8jduzmNAW/83uud4fNpli3I3w8GeeOPK4sqahUkbdcRTvLVbSzTEVe83u6fXeFMpJj1Kx+gprVT1B23fgaHzeGhLa7oc9pX3FWfd+8o9SnObfmHZF91JFCQnYCem3lL5r+/veSnJr8VUHEPIfdpji3Q67gl9Fhs0UcDNjtNu0uN4O9pMJfo+V5nHYlxbqCX8p9lymt8Gvb7goFDnKwvz82m8IHZbEuh2KCB0ruPRIKt8Mul9Muh03a4i3X5h2l+rW4TBX+wEGX4XLYlBrnVrzHGd5Qupx2uYKPnQ5b8EDVp+27zYPUmryfWJdDLoctvGOufvB3sOTHGk4N+2SR7MGdmL3aDq0yYB5U1Cabbe8d6Z5iXHbFuZ3hg+TQTmBXud/coZb6tLO8slbbVZtcDlutr7cYl1022YKJ5cET6YOx2aQEt1OVATNpqIk68W6dkhqrzOQY7dxq17L/rJXfsEWcXKgMHgTs++DYPPj8dUeZNu8wT5hsKi5VReXBv6/VuRy2iDo9LjMBLvMFzMS00n9Y69/jtCvW7ZBhSN4y3yGta7tNinE5VF4ZiDhRciKz26QEj1OJMVUn1QwjdFCo4AmkULISUEWpQ89t/ESxwe94aDvvcthVHErAgreabMuPZ/Fuh1Li3MEEyqkdpT5tLi6Vt6x2tm+JHqdcTrvKff5gYmhtTLqDSW502zCn7l/+Xq23CYcmlNiV+Wr23bTbpMZpcWpWP0Gn1kuQPZjM7S43k9jdFX6VVDtBGe3+vbi0QrHu2EN+nVVIyE5AdeLdyklP0G/FOyWnR7srKsNfFH+wV6embDaFz94lx7rkDyh8RilUT3llQL/tLK9xfXXiq/UgJLgV63ZqV3nwS1fmkzf4BfSW+cLtNgyFz/Rvl+8Q14i5AUhPitEpKbFKT45Ruc+vrbsqtG13hX7fVa7dFeYBW9HOcqmG7yUk3u1QavCMdOjsWElFZThZM9t96O11OuwyT1bZwmfspOBZu+CD8Fl1RzChtlUl1tUTneo7OsMwe4YqKqvOmO/rQDHUW1VtiXsJHQiHei/CvS7BM+WJMS4lxDhlt0nbdldo6y5zff++u0K/76rQrvLKGu2Ey3wBlfkqarTu4twOJcY45bSbO3rzLKm5HkPPHXabXPZqZwirna0M9wBWBoI3f3D5fjnttmpnnV0RvSE2m7Q13PtbEe4FNnsnqt6k22lXfPDsYuhsZqiHuPpBbGKMU26nXVt3lqvQW6ZCb7m27DB7eHeU+g6687PbFDy4NW+xbrPXx+O0y+cPyFtmnv32llaqwm8eDFXf6TntNqXEuZUa51JqvHlf6Te0qbhUm7aXamd5pfk57q7Qml92SLJLWzbV6DM6GJtNqp/oUXpSjAKGodKKqp7Y0GdRWS1mzXVcqV01/OrueTLKHowJh91MJkuDvYshoViorvqBc0qcS26nPdxDEDpLW14ZUMDQXie3Qp9NrNsht8OugKFqPd1VveUBw1BavFvpSTGqnxij9CRP8LG5/Syr9Ov34Hdqa3Bb9vsu8zORzO+nK3iyyuUInrBy2GRIwTPvkXFe7jN7bUI9yoZhBHsJQr3Mhuy2qjPodrst3HvmDxjaVV4ZTrS8ZZWHkETYVLjJW8OyUnKsS3UT3IpzO83ehuB32ekI9TLaFTCMcK9CeaXZo1DhN2PnQImxP1DVixX6XtSW0OcRurkd5rZ6V/AkqGHI7DmvKNWm4tK9Xh/vdqhBSqwaBHvJ3U57uFdwz97+knJ/Va9BWdUJ1p3lldJ+viehE5+h7VFoex7aNrkcdnmr9YJV743wBwx5nPZqPUdV28l4t1PeMp+27Tb3uduD243y4Hq2QqzLEe49NEf7BHv63I5wj5CtWg+xZH5Hq/fGFJeYPeTV49zlsCnWVdV7FOs2t78BwxyZEeoVDX1W/oAht9OuuGC5OHe13vngyQlP6OSzwzyh5XaYJ54qA4Z2l1clMLuDxx67yv3yBwLhE6qhbVzosRHc3lSGeyP36JUM9RAaVXv/0Gie0mDv3e7yyvA2sfqonDi3Q/UTPaqfGKN6iR7VS/QoJc6lTdtL9b/fdul/Rbu0s6xSP/5eoh9/L5HWFR3S5xbndoT3uWbPrzsi3pJjq45XU2LdhxklRxcJ2QnopgtP1YDzG2n+/Pm6/PKL5HK55A8Y4e7z3eWVwQ2CucOvPnynMhBQvNsZ3qjua4hDSGgHHNooH2jDGuN0qG6iW2lx7r2GfRyIL7gDDR2EhYdeBbvzQ0NAqg/HqQieia6b6NYpKXHKTDF3Xq4DLLfM5w8mZxXm2fTKgHwBQ77KgCoDAVX4zccxLodS482hF6lx5lCFfXW9G4ah8sqAdpeb67zCHwgfANqDB4N2u8LTQgmBIzyv9of7HEjoIGR3WbkWLspX586dZXc6I4bdBALm8JhQAnawoT81Uebzy1vmC+847MEhJqGdhxEsU1pRNYSjpKIyHBPxHmfE8JnEGNd+h/xYwTAMeUsrtbuiMjy05EBxWFOlFX5tDWYfToetKsF0RCaaNR02FvocdpZVymm3KTW+aijp/uwo9emX7SXatL1UG3/fpRVr1qlli9Pkdpk9zNUPjm02VR0cBw+MzYNkv/yGoYykGGWmxCozJdY8aRI82DyQylAd1eotr3YA7rDve6hjTeM2EDBUVlkVe2U+v2w2hQ8AahJnofVaWuE3l+90KMZtHlAdiSF9VjMMQ2W+gHYGh9yGhuKWVPjN73ToBFJwG+e02+T3+/XhxwVq3aadfAFbxJDO8uAQufrBA7tQIhrjOjrDkIzg/jG0j/H5jfAokOpDukIcwaP3fY0uONj3MTQEvri0Ijwc1lvqU3KsSw2SzaHKiTGuqN9LRWUgPDSv0m+Ee6vNg3zzRI3TXvNtRnWh4biHuu0tqajU9hKfbFJVYl09wQ6eWDSXUXVSwJBUUeHTwgUL1P2y7nK7zPVSfXh+9bKhIbZGcDhpaMh+bQkdX4USp5NF9ePKcl9AaQluJXgOnFYYhqHfdpbrf0W79L/fdmnD1t2y22zmSUqPs9rJSjOhTTqG9++1jYTsJOGw24Jnu6LfoO+rztBZiUa1Vmuk0JnExJgjtICgGJcjfEBYG2w2W/ggsE6t1HhkOezm2HCnzaUEl1QnwSOXq/ZiZX9C6+hADraBP5bZbDbzbF1c7a7LWLdDjdLiaq2+0OdQP7HmrzG/+8lqlZksn8+netvX6vKLTj0qcSOZPchOh13xniNTvz34O5w4tzPq73BN4vtEYgv+xiTWXfNY8vl8KvzK0MUt6h212KkpWzCRqo2TKAdjt1dtK7KOwE7D7bSHR6bUNpvNJrfz0BO50PerZssIPzLvnHY57Ar+nGB/n8/ROekROr462URzXGmz2VQ/KUb1k2LUvlndI9i648+Jm2oCAAAAwDGOhAwAAAAALGJpQrZz504NGzZMWVlZio2NVfv27bV8+fKIMuvWrdOVV16p5ORkxcfH65xzztHGjRvD88vKyjR48GDVqVNHCQkJ6t27t7Zs2RJRx8aNG9WjRw/FxcWpfv36GjFihCorI39s/P7776tNmzbyeDxq1qyZXnrppSP2vgEAAABAsjghu+mmm5Sfn6+XX35ZX375pbp27aouXbpo0ybzSl3ff/+9OnTooJycHL3//vtas2aNRo4cqZiYqh8U3X333Xrrrbc0d+5cffDBB9q8ebOuvvrq8Hy/368ePXqooqJCy5Yt0//93//ppZde0qhRo8JlNmzYoB49eujiiy/W6tWrNWzYMN10001auHDh0VsZAAAAAE46lv1avrS0VK+//rr+85//qGPHjpKk0aNH66233tL06dP1yCOP6K9//asuv/xyTZw4Mfy6pk2bhh/v2LFDzz//vGbPnq1LLrlEkvTiiy+qZcuW+uSTT3T++edr0aJF+vrrr/Xuu+8qPT1dZ511lsaOHat7771Xo0ePltvt1owZM5Sdna0nnnhCktSyZUt99NFHevLJJ9WtW7ejuFYAAAAAnEwsS8gqKyvl9/sjerskKTY2Vh999JECgYDeeecd3XPPPerWrZtWrVql7Oxs3X///erZs6ckaeXKlfL5fOrSpUv49Tk5OWrcuLEKCgp0/vnnq6CgQLm5uUpPTw+X6datm26//XatXbtWZ599tgoKCiLqCJUZNmzYfttfXl6u8vKqP/Pwes3/UPH5fPL5Dv1/smpbqA3HQltw/CBuEA3iBtEidhAN4gbRONpxcyjLsSwhS0xMVF5ensaOHauWLVsqPT1dr7zyigoKCtSsWTMVFRVp165devTRR/XII49owoQJWrBgga6++mq999576tSpkwoLC+V2u5WSkhJRd3p6ugoLCyVJhYWFEclYaH5o3oHKeL1elZaWKjZ270uhjx8/XmPGjNlr+qJFixQXV3uXoz5c+fn5VjcBxyHiBtEgbhAtYgfRIG4QjaMVNyUlJTUua+kf/Lz88ssaOHCgTjnlFDkcDrVp00b9+vXTypUrFQiYfzJ81VVX6e6775YknXXWWVq2bJlmzJihTp06Wdl03X///Ro+fHj4udfrVaNGjdS1a1clJSVZ2DKTz+dTfn6+Lr300mPuv11w7CJuEA3iBtEidhAN4gbRONpxExo9VxOWJmRNmzbVBx98oN27d8vr9apBgwa67rrrdOqpp6pu3bpyOp06/fTTI14T+n2XJGVkZKiiokLFxcURvWRbtmxRRkZGuMxnn30WUUfoKozVy+x5ZcYtW7YoKSlpn71jkuTxeOTx7P0Hiy6X65jaOBxr7cHxgbhBNIgbRIvYQTSIG0TjaMXNoSzjmPgfsvj4eDVo0EDbt2/XwoULddVVV8ntduucc87R+vXrI8p+++23ysrKkiS1bdtWLpdLixcvDs9fv369Nm7cqLy8PElSXl6evvzySxUVFYXL5OfnKykpKZzs5eXlRdQRKhOqAwAAAACOBEt7yBYuXCjDMNSiRQv973//04gRI5STk6Mbb7xRkjRixAhdd9116tixoy6++GItWLBAb731lt5//31JUnJysgYNGqThw4crLS1NSUlJuvPOO5WXl6fzzz9fktS1a1edfvrpuv766zVx4kQVFhbqwQcf1ODBg8M9XLfddpumTZume+65RwMHDtSSJUs0Z84cvfPOO5asFwAAAAAnB0sTsh07duj+++/XL7/8orS0NPXu3Vvjxo0Ld/H16tVLM2bM0Pjx4zV06FC1aNFCr7/+ujp06BCu48knn5Tdblfv3r1VXl6ubt266ZlnngnPdzgcevvtt3X77bcrLy9P8fHxGjBggB5++OFwmezsbL3zzju6++67NWXKFDVs2FDPPfccl7wHAAAAcERZmpBde+21uvbaaw9YZuDAgRo4cOB+58fExOjpp5/W008/vd8yWVlZmj9//gGXc9FFF2nVqlUHbjAAAAAA1KJj4jdkAAAAAHAyIiEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARSxPyHbu3Klhw4YpKytLsbGxat++vZYvX77PsrfddptsNpsmT54cMX3btm3q37+/kpKSlJKSokGDBmnXrl0RZdasWaMLL7xQMTExatSokSZOnLhX/XPnzlVOTo5iYmKUm5ur+fPn19r7BAAAAIA9WZ6Q3XTTTcrPz9fLL7+sL7/8Ul27dlWXLl20adOmiHJvvPGGPvnkE2VmZu5VR//+/bV27Vrl5+fr7bff1tKlS3XLLbeE53u9XnXt2lVZWVlauXKlHnvsMY0ePVozZ84Ml1m2bJn69eunQYMGadWqVerZs6d69uypr7766si9eQAAAAAnNUsTstLSUr3++uuaOHGiOnbsqGbNmmn06NFq1qyZpk+fHi63adMm3XnnnZo1a5ZcLldEHevWrdOCBQv03HPP6bzzzlOHDh00depUvfrqq9q8ebMkadasWaqoqNALL7ygVq1aqW/fvho6dKgmTZoUrmfKlCnq3r27RowYoZYtW2rs2LFq06aNpk2bdnRWBgAAAICTjtPKhVdWVsrv9ysmJiZiemxsrD766CNJUiAQ0PXXX68RI0aoVatWe9VRUFCglJQUtWvXLjytS5custvt+vTTT9WrVy8VFBSoY8eOcrvd4TLdunXThAkTtH37dqWmpqqgoEDDhw+PqLtbt26aN2/ePtteXl6u8vLy8HOv1ytJ8vl88vl8h7YijoBQG46FtuD4QdwgGsQNokXsIBrEDaJxtOPmUJZjaUKWmJiovLw8jR07Vi1btlR6erpeeeUVFRQUqFmzZpKkCRMmyOl0aujQofuso7CwUPXr14+Y5nQ6lZaWpsLCwnCZ7OzsiDLp6enheampqSosLAxPq14mVMeexo8frzFjxuw1fdGiRYqLi6vBuz868vPzrW4CjkPEDaJB3CBaxA6iQdwgGkcrbkpKSmpc1tKETJJefvllDRw4UKeccoocDofatGmjfv36aeXKlVq5cqWmTJmizz//XDabzeqmRrj//vsjetS8Xq8aNWqkrl27KikpycKWmXw+n/Lz83XppZfuNcwT2B/iBtEgbhAtYgfRIG4QjaMdN6HRczVheULWtGlTffDBB9q9e7e8Xq8aNGig6667Tqeeeqo+/PBDFRUVqXHjxuHyfr9ff/7znzV58mT9+OOPysjIUFFRUUSdlZWV2rZtmzIyMiRJGRkZ2rJlS0SZ0PODlQnN35PH45HH49lrusvlOqY2Dsdae3B8IG4QDeIG0SJ2EA3iBtE4WnFzKMuw/CqLIfHx8WrQoIG2b9+uhQsX6qqrrtL111+vNWvWaPXq1eFbZmamRowYoYULF0qS8vLyVFxcrJUrV4brWrJkiQKBgM4777xwmaVLl0aM5czPz1eLFi2UmpoaLrN48eKINuXn5ysvL+9Iv3UAAAAAJynLe8gWLlwowzDUokUL/e9//9OIESOUk5OjG2+8US6XS3Xq1Iko73K5lJGRoRYtWkiSWrZsqe7du+vmm2/WjBkz5PP5NGTIEPXt2zd8ifw//vGPGjNmjAYNGqR7771XX331laZMmaInn3wyXO9dd92lTp066YknnlCPHj306quvasWKFRGXxgcAAACA2mR5D9mOHTs0ePBg5eTk6E9/+pM6dOighQsXHlI336xZs5STk6POnTvr8ssvV4cOHSISqeTkZC1atEgbNmxQ27Zt9ec//1mjRo2K+K+y9u3ba/bs2Zo5c6bOPPNMvfbaa5o3b57OOOOMWn2/AAAAABBieQ/Ztddeq2uvvbbG5X/88ce9pqWlpWn27NkHfF3r1q314YcfHrBMnz591KdPnxq3BQAAAAAOh+U9ZAAAAABwsiIhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEUsT8h27typYcOGKSsrS7GxsWrfvr2WL18uSfL5fLr33nuVm5ur+Ph4ZWZm6k9/+pM2b94cUce2bdvUv39/JSUlKSUlRYMGDdKuXbsiyqxZs0YXXnihYmJi1KhRI02cOHGvtsydO1c5OTmKiYlRbm6u5s+ff+TeOAAAAICTnuUJ2U033aT8/Hy9/PLL+vLLL9W1a1d16dJFmzZtUklJiT7//HONHDlSn3/+uf79739r/fr1uvLKKyPq6N+/v9auXav8/Hy9/fbbWrp0qW655ZbwfK/Xq65duyorK0srV67UY489ptGjR2vmzJnhMsuWLVO/fv00aNAgrVq1Sj179lTPnj311VdfHbV1AQAAAODk4rRy4aWlpXr99df1n//8Rx07dpQkjR49Wm+99ZamT5+uRx55RPn5+RGvmTZtms4991xt3LhRjRs31rp167RgwQItX75c7dq1kyRNnTpVl19+uR5//HFlZmZq1qxZqqio0AsvvCC3261WrVpp9erVmjRpUjhxmzJlirp3764RI0ZIksaOHav8/HxNmzZNM2bMOIprBQAAAMDJwtKErLKyUn6/XzExMRHTY2Nj9dFHH+3zNTt27JDNZlNKSookqaCgQCkpKeFkTJK6dOkiu92uTz/9VL169VJBQYE6duwot9sdLtOtWzdNmDBB27dvV2pqqgoKCjR8+PCIZXXr1k3z5s3bZzvKy8tVXl4efu71eiWZwyx9Pl+N18GREmrDsdAWHD+IG0SDuEG0iB1Eg7hBNI523BzKcixNyBITE5WXl6exY8eqZcuWSk9P1yuvvKKCggI1a9Zsr/JlZWW699571a9fPyUlJUmSCgsLVb9+/YhyTqdTaWlpKiwsDJfJzs6OKJOenh6el5qaqsLCwvC06mVCdexp/PjxGjNmzF7TFy1apLi4uBqugSNvzx5GoCaIG0SDuEG0iB1Eg7hBNI5W3JSUlNS4rKUJmSS9/PLLGjhwoE455RQ5HA61adNG/fr108qVKyPK+Xw+XXvttTIMQ9OnT7eotVXuv//+iB41r9erRo0aqWvXruFk0Uo+n0/5+fm69NJL5XK5rG4OjhPEDaJB3CBaxA6iQdwgGkc7bkKj52rC8oSsadOm+uCDD7R79255vV41aNBA1113nU499dRwmVAy9tNPP2nJkiURCU9GRoaKiooi6qysrNS2bduUkZERLrNly5aIMqHnBysTmr8nj8cjj8ez13SXy3VMbRyOtfbg+EDcIBrEDaJF7CAaxA2icbTi5lCWYflVFkPi4+PVoEEDbd++XQsXLtRVV10lqSoZ++677/Tuu++qTp06Ea/Ly8tTcXFxRI/akiVLFAgEdN5554XLLF26NGIsZ35+vlq0aKHU1NRwmcWLF0fUnZ+fr7y8vCPyfgEAAADA8oRs4cKFWrBggTZs2KD8/HxdfPHFysnJ0Y033iifz6drrrlGK1as0KxZs+T3+1VYWKjCwkJVVFRIklq2bKnu3bvr5ptv1meffaaPP/5YQ4YMUd++fZWZmSlJ+uMf/yi3261BgwZp7dq1+te//qUpU6ZEDDm86667tGDBAj3xxBP65ptvNHr0aK1YsUJDhgyxZL0AAAAAOPFZnpDt2LFDgwcPVk5Ojv70pz+pQ4cOWrhwoVwulzZt2qQ333xTv/zyi8466yw1aNAgfFu2bFm4jlmzZiknJ0edO3fW5Zdfrg4dOkT8x1hycrIWLVqkDRs2qG3btvrzn/+sUaNGRfxXWfv27TV79mzNnDlTZ555pl577TXNmzdPZ5xxxlFdHwAAAABOHpb/huzaa6/Vtddeu895TZo0kWEYB60jLS1Ns2fPPmCZ1q1b68MPPzxgmT59+qhPnz4HXR4AAAAA1AbLe8gAAAAA4GRFQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLWJ6Q7dy5U8OGDVNWVpZiY2PVvn17LV++PDzfMAyNGjVKDRo0UGxsrLp06aLvvvsuoo5t27apf//+SkpKUkpKigYNGqRdu3ZFlFmzZo0uvPBCxcTEqFGjRpo4ceJebZk7d65ycnIUExOj3NxczZ8//8i8aQAAAADQMZCQ3XTTTcrPz9fLL7+sL7/8Ul27dlWXLl20adMmSdLEiRP11FNPacaMGfr0008VHx+vbt26qaysLFxH//79tXbtWuXn5+vtt9/W0qVLdcstt4Tne71ede3aVVlZWVq5cqUee+wxjR49WjNnzgyXWbZsmfr166dBgwZp1apV6tmzp3r27Kmvvvrq6K0MAAAAACcVSxOy0tJSvf7665o4caI6duyoZs2aafTo0WrWrJmmT58uwzA0efJkPfjgg7rqqqvUunVr/eMf/9DmzZs1b948SdK6deu0YMECPffcczrvvPPUoUMHTZ06Va+++qo2b94sSZo1a5YqKir0wgsvqFWrVurbt6+GDh2qSZMmhdsyZcoUde/eXSNGjFDLli01duxYtWnTRtOmTbNi1QAAAAA4CTitXHhlZaX8fr9iYmIipsfGxuqjjz7Shg0bVFhYqC5duoTnJScn67zzzlNBQYH69u2rgoICpaSkqF27duEyXbp0kd1u16effqpevXqpoKBAHTt2lNvtDpfp1q2bJkyYoO3btys1NVUFBQUaPnx4RDu6desWTvz2VF5ervLy8vBzr9crSfL5fPL5fFGvk9oSasOx0BYcP4gbRIO4QbSIHUSDuEE0jnbcHMpyLE3IEhMTlZeXp7Fjx6ply5ZKT0/XK6+8ooKCAjVr1kyFhYWSpPT09IjXpaenh+cVFhaqfv36EfOdTqfS0tIiymRnZ+9VR2heamqqCgsLD7icPY0fP15jxozZa/qiRYsUFxdX01VwxOXn51vdBByHiBtEg7hBtIgdRIO4QTSOVtyUlJTUuKylCZkkvfzyyxo4cKBOOeUUORwOtWnTRv369dPKlSutbtoB3X///RE9al6vV40aNVLXrl2VlJRkYctMPp9P+fn5uvTSS+VyuaxuDo4TxA2iQdwgWsQOokHcIBpHO25Co+dqwvKErGnTpvrggw+0e/dueb1eNWjQQNddd51OPfVUZWRkSJK2bNmiBg0ahF+zZcsWnXXWWZKkjIwMFRUVRdRZWVmpbdu2hV+fkZGhLVu2RJQJPT9YmdD8PXk8Hnk8nr2mu1yuY2rjcKy1B8cH4gbRIG4QLWIH0SBuEI2jFTeHsgzLE7KQ+Ph4xcfHa/v27Vq4cKEmTpyo7OxsZWRkaPHixeEEzOv16tNPP9Xtt98uScrLy1NxcbFWrlyptm3bSpKWLFmiQCCg8847L1zmr3/9q3w+X3jl5Ofnq0WLFkpNTQ2XWbx4sYYNGxZuU35+vvLy8o7SGgAAAMCJzu/38/s3C/h8PjmdTpWVlcnv99dKnW63W3b74V8j0fKEbOHChTIMQy1atND//vc/jRgxQjk5Obrxxhtls9k0bNgwPfLII2revLmys7M1cuRIZWZmqmfPnpKkli1bqnv37rr55ps1Y8YM+Xw+DRkyRH379lVmZqYk6Y9//KPGjBmjQYMG6d5779VXX32lKVOm6Mknnwy346677lKnTp30xBNPqEePHnr11Ve1YsWKiEvjAwAAANEwDEOFhYUqLi62uiknJcMwlJGRoZ9//lk2m61W6rTb7crOzo64cGA0LE/IduzYofvvv1+//PKL0tLS1Lt3b40bNy7ck3XPPfdo9+7duuWWW1RcXKwOHTpowYIFEVdmnDVrloYMGaLOnTvLbrerd+/eeuqpp8Lzk5OTtWjRIg0ePFht27ZV3bp1NWrUqIj/Kmvfvr1mz56tBx98UA888ICaN2+uefPm6Ywzzjh6KwMAAAAnpFAyVr9+fcXFxdVaUoCaCQQC2rVrlxISEmqlVysQCGjz5s369ddf1bhx48P6PC1PyK699lpde+21+51vs9n08MMP6+GHH95vmbS0NM2ePfuAy2ndurU+/PDDA5bp06eP+vTpc+AGAwAAAIfA7/eHk7E6depY3ZyTUiAQUEVFhWJiYmolIZOkevXqafPmzaqsrDys36VZ+sfQAAAAwIku9JuxY+mvkXD4QkMVD/c3aSRkAAAAwFHAMMUTS639Fq1WagEAAAAAHDISMgAAAABHTZMmTTR58mSrm3HMICEDAAAAsBebzXbA2+jRo6Oqd/ny5RFXO4/GRRddFPH/wcczy6+yCAAAAODY8+uvv4Yf/+tf/9KoUaO0fv368LSEhITwY8Mw5Pf75XQePL2oV69e7Tb0OEcPGQAAAIC9ZGRkhG/Jycmy2Wzh5998840SExP13//+V23btpXH49FHH32k77//XldddZXS09OVkJCgc845R++++25EvXsOWbTZbHruuefUq1cvxcXFqXnz5nrzzTcPq+2vv/66WrVqJY/HoyZNmmjSpEkR85955hk1b95cMTExSk9P1zXXXBOe99prryk3N1exsbGqU6eOunTpot27dx9Wew6EHjIAAADgKDMMQ6W+w7tcerRiXY5au0Lgfffdp8cff1ynnnqqUlNT9fPPP+vyyy/XuHHj5PF49I9//ENXXHGF1q9fr8aNG++3njFjxmjixIl67LHHNHXqVPXv318//fST0tLSDrlNK1eu1LXXXqvRo0fruuuu07Jly3THHXcoLi5Ot912m1asWKGhQ4fq5ZdfVvv27bVt27bw/xX/+uuv6tevnyZOnKhevXpp586d+vDDD2UYRtTr6GBIyAAAAICjrNTn1+mjFlqy7K8f7qY4d+2kAQ8//LAuvfTS8PO0tDSdeeaZ4edjx47VG2+8oTfffFNDhgzZbz033HCD+vXrJ0n629/+pqeeekqfffaZunfvfshtmjRpkjp37qyRI0dKkk477TStXbtWU6dO1W233aaNGzcqPj5ef/jDH5SYmKisrCydffbZksyErLKyUldffbWysrIkSbm5uYfchkMR1ZDFn3/+Wb/88kv4+WeffaZhw4Zp5syZtdYwAAAAAMe2du3aRTzftWuX/vKXv6hly5ZKSUlRQkKC1q1bp40bNx6wntatW4cfx8fHKykpSUVFRVG1ad26dbrgggsiprVv317ff/+9/H6/Lr30UmVlZenUU0/V9ddfr1mzZqmkpESSdOaZZ6pz587Kzc1Vnz599Oyzz2r79u1RtaOmokqN//jHP+qWW27R9ddfr8LCQl166aVq1aqVZs2apcLCQo0aNaq22wkAAACcMGJdDn39cDfLll1b4uPjI57/5S9/UX5+vh5//HE1a9ZMsbGxuuaaa1RRUXHAelwuV8Rzm82mQCBQa+2sLjExUZ9//rnef/99LVq0SKNGjdLo0aO1fPlypaSkKD8/X8uWLdOiRYs0depU/fWvf9Wnn36q7OzsI9KeqHrIvvrqK5177rmSpDlz5uiMM87QsmXLNGvWLL300ku12T4AAADghGOz2RTndlpyq63fj+3Lxx9/rBtuuEG9evVSbm6uMjIy9OOPPx6x5e1Ly5Yt9fHHH0dMW7ZsmZo2bSqHw0xGnU6nunTpookTJ2rNmjX68ccftWTJEknmZ3PBBRdozJgxWrVqldxut954440j1t6oesh8Pp88Ho8k6d1339WVV14pScrJyYm4PCYAAACAk0fz5s3173//W1dccYVsNptGjhx5xHq6fvvtN61evTpiWoMGDfTnP/9Z55xzjsaOHavrrrtOBQUFevrpp/X4449Lkt5++2398MMP6tixo1JTUzV//nwFAgG1aNFCn376qRYvXqyuXbuqfv36+vTTT/Xbb7+pZcuWR+Q9SFH2kLVq1UozZszQhx9+qPz8/PCP7TZv3qw6derUagMBAAAAHB8mTZqk1NRUtW/fXldccYW6deumNm3aHJFlzZ49W2effXbE7dlnn1WbNm00Z84cvfrqqzrjjDM0atQojRkzRn/84x8lSSkpKfr3v/+tSy65RC1bttSMGTP0yiuvqFWrVkpKStLSpUt1+eWX67TTTtODDz6oJ554QpdddtkReQ9SlD1kEyZMUK9evfTYY49pwIAB4SupvPnmm+GhjAAAAABODDfccINuuOGG8POLLrpon5eCb9KkSXjoX8jgwYMjnu85hHFf9RQXFx+wPe+///4B5/fu3Vu9e/cOPw8EAvJ6vZKkDh067Pf1LVu21IIFCw5Yd22LKiG76KKLtHXrVnm9XqWmpoan33LLLYqLi6u1xgEAAADAiSyqIYulpaUqLy8PJ2M//fSTJk+erPXr16t+/fq12kAAAAAAOFFFlZBdddVV+sc//iHJ7E4877zz9MQTT6hnz56aPn16rTYQAAAAAE5UUSVkn3/+uS688EJJ0muvvab09HT99NNP+sc//qGnnnqqVhsIAAAAACeqqBKykpISJSYmSpIWLVqkq6++Wna7Xeeff75++umnWm0gAAAAAJyookrImjVrpnnz5unnn3/WwoUL1bVrV0lSUVGRkpKSarWBAAAAAHCiiiohGzVqlP7yl7+oSZMmOvfcc5WXlyfJ7C07++yza7WBAAAAAHCiiuqy99dcc406dOigX3/9NfwfZJLUuXNn9erVq9YaBwAAAAAnsqgSMknKyMhQRkaGfvnlF0lSw4YN+VNoAAAAADgEUQ1ZDAQCevjhh5WcnKysrCxlZWUpJSVFY8eOVSAQqO02AgAAAMAJKaqE7K9//aumTZumRx99VKtWrdKqVav0t7/9TVOnTtXIkSNru40AAAAAjjKbzXbA2+jRow+r7nnz5tVaueNZVEMW/+///k/PPfecrrzyyvC01q1b65RTTtEdd9yhcePG1VoDAQAAABx9v/76a/jxv/71L40aNUrr168PT0tISLCiWSecqHrItm3bppycnL2m5+TkaNu2bYfdKAAAAADWCl0zIiMjQ8nJybLZbBHTXn31VbVs2VIxMTHKycnRM888E35tRUWFhgwZogYNGigmJkZZWVkaP368JKlJkyaSpF69eslms4WfH6rQz6gaNmwoj8ejs846SwsWLNhnG+Li4pSbm6tHH31UkmQYhkaPHq3GjRvL4/EoMzNTQ4cOjW5FHaaoesjOPPNMTZs2TU899VTE9GnTpql169a10jAAAADghGUYkq/EmmW74iSb7bCqmDVrlkaNGqVp06bp7LPP1qpVq3TzzTcrPj5eAwYM0FNPPaU333xTc+bMUePGjfXzzz/r559/liQtX75c9evX14svvqju3bvL4XBE1YYpU6boiSee0N///nedffbZeuGFF3TllVdq7dq1at68eUQbGjZsqG+++SbcefT666/rySef1KuvvqpWrVqpsLBQX3zxxWGtk2hFlZBNnDhRPXr00Lvvvhv+D7KCggL9/PPPmj9/fq02EAAAADjh+Eqkv2Vas+wHNkvu+MOq4qGHHtITTzyhq6++WpKUnZ2tr7/+Wn//+981YMAAbdy4Uc2bN1eHDh1ks9mUlZUVfm29evUkSSkpKcrIyIi6DY8//rjuvfde9e3bV5I0YcIEvffee5o8ebKefvrpiDYYhqHU1FQlJSVJkjZu3KiMjAx16dJFLpdLjRs3tuyK8VENWezUqZO+/fZb9erVS8XFxSouLtbVV1+ttWvX6uWXX67tNgIAAAA4RuzevVvff/+9Bg0apISEhPDtkUce0ffffy9JuuGGG7R69Wq1aNFCQ4cO1aJFi2q1DV6vV5s3b9YFF1wQMf2CCy7QunXr9mrDXXfdpSVLloTL9enTR6WlpTr11FN1880364033lBlZWWttrGmov4fsszMzL0u3vHFF1/o+eef18yZMw+7YQAAAMAJyxVn9lRZtezDsGvXLknSs88+q/POOy9iXmj4YZs2bbRhwwb997//1bvvvqtrr71WXbp00WuvvXZYyz4U1duQn5+vG2+8UbNmzdLrr7+uRo0aaf369Xr33XeVn5+vO+64Q4899pg++OADuVyuo9ZG6TASMgAAAABRstkOe9igVdLT05WZmakffvhB/fv332+5pKQkXXfddbruuut0zTXXqHv37tq2bZvS0tLkcrnk9/ujbkNSUpIyMzP18ccfq1OnTuHpH3/8ccTQw1Ab+vTpo8suu0zXXHNNuA2xsbG64oordMUVV2jw4MHKycnRl19+qTZt2kTdrmiQkAEAAAA4JGPGjNHQoUOVnJys7t27q7y8XCtWrND27ds1fPhwTZo0SQ0aNNDZZ58tu92uuXPnKiMjQykpKZLMKy0uXrxYF1xwgTwej1JTU/e7rA0bNmj16tUR05o3b64RI0booYceUtOmTXXWWWfpxRdf1OrVqzVr1ixJimiDJP3nP/8Jt+Gll16S3+/Xeeedp7i4OP3zn/9UbGxsxG/djhYSMgAAAACH5KabblJcXJwee+wxjRgxQvHx8crNzdWwYcMkSYmJiZo4caK+++47ORwOnXPOOZo/f77sdvMSFk888YSGDx+uZ599Vqeccop+/PHH/S5r+PDhe0378MMPNXToUO3YsUN//vOfVVRUpNNPP11vvvmmmjdvvs82nH322Xr77bdlt9uVkpKiRx99VMOHD5ff71dubq7eeust1alTp9bX1cHYDMMwalo4dBWV/SkuLtYHH3xwWN2Pxyuv16vk5GTt2LEjfPUWK/l8Ps2fP1+XX375UR8Hi+MXcYNoEDeIFrGDaByPcVNWVqYNGzYoOztbMTExVjfnpBQIBOT1epWUlBROCg/XgT7XQ8kNDqk1ycnJB7xlZWXpT3/6U43r8/v9GjlypLKzsxUbG6umTZtq7Nixqp4j7tq1S0OGDFHDhg0VGxur008/XTNmzIiop6ysTIMHD1adOnWUkJCg3r17a8uWLRFlNm7cqB49eiguLk7169fXiBEj9rqSyvvvv682bdrI4/GoWbNmeumllw5l9QAAAADAITmkIYsvvvhirS58woQJmj59uv7v//5PrVq10ooVK3TjjTcqOTk5/E/Zw4cP15IlS/TPf/5TTZo00aJFi3THHXcoMzNTV155pSTp7rvv1jvvvKO5c+cqOTlZQ4YM0dVXX62PP/5Ykpn49ejRQxkZGVq2bJl+/fVX/elPf5LL5dLf/vY3SebY1B49eui2227TrFmztHjxYt10001q0KCBunXrVqvvGwAAAACkKP+HrLYsW7ZMV111lXr06KEmTZrommuuUdeuXfXZZ59FlBkwYIAuuugiNWnSRLfccovOPPPMcJkdO3bo+eef16RJk3TJJZeobdu2evHFF7Vs2TJ98sknkqRFixbp66+/1j//+U+dddZZuuyyyzR27Fg9/fTTqqiokCTNmDFD2dnZeuKJJ9SyZUsNGTJE11xzjZ588smjv2IAAAAAnBQsvahH+/btNXPmTH377bc67bTT9MUXX+ijjz7SpEmTIsq8+eabGjhwoDIzM/X+++/r22+/DSdKK1eulM/nU5cuXcKvycnJUePGjVVQUKDzzz9fBQUFys3NVXp6erhMt27ddPvtt2vt2rU6++yzVVBQEFFHqEzoh4l7Ki8vV3l5efi51+uVZI5r9vl8h71uDleoDcdCW3D8IG4QDeIG0SJ2EI3jMW58Pp8Mw1AgEFAgELC6OSel0E+iQp9DbQgEAjIMQz6fL/z/ayGHEp+WJmT33XefvF6vcnJy5HA45Pf7NW7cuIj/M5g6dapuueUWNWzYUE6nU3a7Xc8++6w6duwoSSosLJTb7Q5fQjMkPT1dhYWF4TLVk7HQ/NC8A5Xxer0qLS1VbGxsxLzx48drzJgxe72nRYsWKS7u8P5srzbl5+db3QQch4gbRIO4QbSIHUTjeIobp9OpjIwM7dy5Mzw6C9bYuXNnrdVVUVGh0tJSLV26dK9rU5SUlNS4HksTsjlz5mjWrFmaPXu2WrVqpdWrV2vYsGHKzMzUgAEDJJkJ2SeffKI333xTWVlZWrp0qQYPHqzMzMy9erSOpvvvvz/iEpxer1eNGjVS165dj5mrLObn5+vSSy89bq5ABOsRN4gGcYNoETuIxvEYN36/Xz/88IPsdvsxcZx4MjIMQzt37lRiYqJsNlut1On1ehUbG6tLLrlETqdzr3k1ZWlCNmLECN13333q27evJCk3N1c//fSTxo8frwEDBqi0tFQPPPCA3njjDfXo0UOS1Lp1a61evVqPP/64unTpooyMDFVUVKi4uDiil2zLli3KyMiQJGVkZET8Li00PzQvdL/nlRm3bNmipKSkvXrHJMnj8cjj8ew13eVyHVMbh2OtPTg+EDeIBnGDaBE7iMbxFDcul0upqanaunWr7Ha74uLiai0pQM0EAgFVVFSovLy8Vi57HwgEtHXrVsXHxysmJmavz/NQYtPShKykpGSvFeJwOMLjOkO/xzpQmbZt28rlcmnx4sXq3bu3JGn9+vXauHGj8vLyJEl5eXkaN26cioqKVL9+fUlmN3dSUpJOP/30cJn58+dHLCc/Pz9cBwAAABCtUCdAUVGRxS05ORmGEf4ZUm0lw3a7XY0bNz7s+ixNyK644gqNGzdOjRs3VqtWrbRq1SpNmjRJAwcOlCQlJSWpU6dOGjFihGJjY5WVlaUPPvhA//jHP8IX/khOTtagQYM0fPhwpaWlKSkpSXfeeafy8vJ0/vnnS5K6du2q008/Xddff70mTpyowsJCPfjggxo8eHC4l+u2227TtGnTdM8992jgwIFasmSJ5syZo3feecealQMAAIAThs1mU4MGDVS/fv3j6oIkJwqfz6elS5eqY8eOtdaz6na7a6W3zdKEbOrUqRo5cqTuuOMOFRUVKTMzU7feeqtGjRoVLvPqq6/q/vvvV//+/bVt2zZlZWVp3Lhxuu2228JlnnzySdntdvXu3Vvl5eXq1q2bnnnmmfB8h8Oht99+W7fffrvy8vIUHx+vAQMG6OGHHw6Xyc7O1jvvvKO7775bU6ZMUcOGDfXcc8/xH2QAAACoNQ6HY68r8uHIczgcqqysVExMzDE31NXShCwxMVGTJ0/W5MmT91smIyPjoH9IHRMTo6efflpPP/30fstkZWXtNSRxTxdddJFWrVp1wDIAAAAAUFss/WNoAAAAADiZkZABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIpYmZH6/XyNHjlR2drZiY2PVtGlTjR07VoZhRJRbt26drrzySiUnJys+Pl7nnHOONm7cGJ5fVlamwYMHq06dOkpISFDv3r21ZcuWiDo2btyoHj16KC4uTvXr19eIESNUWVkZUeb9999XmzZt5PF41KxZM7300ktH7L0DAAAAgKUJ2YQJEzR9+nRNmzZN69at04QJEzRx4kRNnTo1XOb7779Xhw4dlJOTo/fff19r1qzRyJEjFRMTEy5z991366233tLcuXP1wQcfaPPmzbr66qvD8/1+v3r06KGKigotW7ZM//d//6eXXnpJo0aNCpfZsGGDevTooYsvvlirV6/WsGHDdNNNN2nhwoVHZ2UAAAAAOOk4rVz4smXLdNVVV6lHjx6SpCZNmuiVV17RZ599Fi7z17/+VZdffrkmTpwYnta0adPw4x07duj555/X7Nmzdckll0iSXnzxRbVs2VKffPKJzj//fC1atEhff/213n33XaWnp+uss87S2LFjde+992r06NFyu92aMWOGsrOz9cQTT0iSWrZsqY8++khPPvmkunXrdjRWBwAAAICTjKUJWfv27TVz5kx9++23Ou200/TFF1/oo48+0qRJkyRJgUBA77zzju655x5169ZNq1atUnZ2tu6//3717NlTkrRy5Ur5fD516dIlXG9OTo4aN26sgoICnX/++SooKFBubq7S09PDZbp166bbb79da9eu1dlnn62CgoKIOkJlhg0bts+2l5eXq7y8PPzc6/VKknw+n3w+X22snsMSasOx0BYcP4gbRIO4QbSIHUSDuEE0jnbcHMpyLE3I7rvvPnm9XuXk5MjhcMjv92vcuHHq37+/JKmoqEi7du3So48+qkceeUQTJkzQggULdPXVV+u9995Tp06dVFhYKLfbrZSUlIi609PTVVhYKEkqLCyMSMZC80PzDlTG6/WqtLRUsbGxEfPGjx+vMWPG7PWeFi1apLi4uOhXSi3Lz8+3ugk4DhE3iAZxg2gRO4gGcYNoHK24KSkpqXFZSxOyOXPmaNasWZo9e7ZatWoV/u1WZmamBgwYoEAgIEm66qqrdPfdd0uSzjrrLC1btkwzZsxQp06dLGv7/fffr+HDh4efe71eNWrUSF27dlVSUpJl7Qrx+XzKz8/XpZdeKpfLZXVzcJwgbhAN4gbRInYQDeIG0TjacRMaPVcTliZkI0aM0H333ae+fftKknJzc/XTTz9p/PjxGjBggOrWrSun06nTTz894nWh33dJUkZGhioqKlRcXBzRS7ZlyxZlZGSEy1T/XVpofmhe6H7PKzNu2bJFSUlJe/WOSZLH45HH49lrusvlOqY2Dsdae3B8IG4QDeIG0SJ2EA3iBtE4WnFzKMuw9CqLJSUlstsjm+BwOMI9Y263W+ecc47Wr18fUebbb79VVlaWJKlt27ZyuVxavHhxeP769eu1ceNG5eXlSZLy8vL05ZdfqqioKFwmPz9fSUlJ4WQvLy8voo5QmVAdAAAAAFDbLO0hu+KKKzRu3Dg1btxYrVq10qpVqzRp0iQNHDgwXGbEiBG67rrr1LFjR1188cVasGCB3nrrLb3//vuSpOTkZA0aNEjDhw9XWlqakpKSdOeddyovL0/nn3++JKlr1646/fTTdf3112vixIkqLCzUgw8+qMGDB4d7uW677TZNmzZN99xzjwYOHKglS5Zozpw5euedd476egEAAABwcrA0IZs6dapGjhypO+64Q0VFRcrMzNStt94a8f9gvXr10owZMzR+/HgNHTpULVq00Ouvv64OHTqEyzz55JOy2+3q3bu3ysvL1a1bNz3zzDPh+Q6HQ2+//bZuv/125eXlKT4+XgMGDNDDDz8cLpOdna133nlHd999t6ZMmaKGDRvqueee45L3AAAAAI4YSxOyxMRETZ48WZMnTz5guYEDB0b0mu0pJiZGTz/9tJ5++un9lsnKytL8+fMPuJyLLrpIq1atOmAZAAAAAKgtlv6GDAAAAABOZiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsIilCZnf79fIkSOVnZ2t2NhYNW3aVGPHjpVhGPssf9ttt8lms2ny5MkR07dt26b+/fsrKSlJKSkpGjRokHbt2hVRZs2aNbrwwgsVExOjRo0aaeLEiXvVP3fuXOXk5CgmJka5ubmaP39+rb1XAAAAANiTpQnZhAkTNH36dE2bNk3r1q3ThAkTNHHiRE2dOnWvsm+88YY++eQTZWZm7jWvf//+Wrt2rfLz8/X2229r6dKluuWWW8LzvV6vunbtqqysLK1cuVKPPfaYRo8erZkzZ4bLLFu2TP369dOgQYO0atUq9ezZUz179tRXX311ZN48AAAAgJOepQnZsmXLdNVVV6lHjx5q0qSJrrnmGnXt2lWfffZZRLlNmzbpzjvv1KxZs+RyuSLmrVu3TgsWLNBzzz2n8847Tx06dNDUqVP16quvavPmzZKkWbNmqaKiQi+88IJatWqlvn37aujQoZo0aVK4nilTpqh79+4aMWKEWrZsqbFjx6pNmzaaNm3akV8RAAAAAE5KTisX3r59e82cOVPffvutTjvtNH3xxRf66KOPIhKlQCCg66+/XiNGjFCrVq32qqOgoEApKSlq165deFqXLl1kt9v16aefqlevXiooKFDHjh3ldrvDZbp166YJEyZo+/btSk1NVUFBgYYPHx5Rd7du3TRv3rx9tr28vFzl5eXh516vV5Lk8/nk8/miWh+1KdSGY6EtOH4QN4gGcYNoETuIBnGDaBztuDmU5ViakN13333yer3KycmRw+GQ3+/XuHHj1L9//3CZCRMmyOl0aujQofuso7CwUPXr14+Y5nQ6lZaWpsLCwnCZ7OzsiDLp6enheampqSosLAxPq14mVMeexo8frzFjxuw1fdGiRYqLizvIOz968vPzrW4CjkPEDaJB3CBaxA6iQdwgGkcrbkpKSmpc1tKEbM6cOZo1a5Zmz56tVq1aafXq1Ro2bJgyMzM1YMAArVy5UlOmTNHnn38um81mZVP3cv/990f0qHm9XjVq1Ehdu3ZVUlKShS0z+Xw+5efn69JLL91rmCewP8QNokHcIFrEDqJB3CAaRztuQqPnasLShGzEiBG677771LdvX0lSbm6ufvrpJ40fP14DBgzQhx9+qKKiIjVu3Dj8Gr/frz//+c+aPHmyfvzxR2VkZKioqCii3srKSm3btk0ZGRmSpIyMDG3ZsiWiTOj5wcqE5u/J4/HI4/HsNd3lch1TG4djrT04PhA3iAZxg2gRO4gGcYNoHK24OZRlWHpRj5KSEtntkU1wOBwKBAKSpOuvv15r1qzR6tWrw7fMzEyNGDFCCxculCTl5eWpuLhYK1euDNexZMkSBQIBnXfeeeEyS5cujRjLmZ+frxYtWig1NTVcZvHixRFtyc/PV15eXu2/cQAAAACQxT1kV1xxhcaNG6fGjRurVatWWrVqlSZNmqSBAwdKkurUqaM6depEvMblcikjI0MtWrSQJLVs2VLdu3fXzTffrBkzZsjn82nIkCHq27dv+BL5f/zjHzVmzBgNGjRI9957r7766itNmTJFTz75ZLjeu+66S506ddITTzyhHj166NVXX9WKFSsiLo0PAAAAALXJ0oRs6tSpGjlypO644w4VFRUpMzNTt956q0aNGnVI9cyaNUtDhgxR586dZbfb1bt3bz311FPh+cnJyVq0aJEGDx6stm3bqm7duho1alTEf5W1b99es2fP1oMPPqgHHnhAzZs317x583TGGWfU2vsFAAAAgOosTcgSExM1efJkTZ48ucav+fHHH/ealpaWptmzZx/wda1bt9aHH354wDJ9+vRRnz59atwWAAAAADgclv6GDAAAAABOZiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsIilCZnf79fIkSOVnZ2t2NhYNW3aVGPHjpVhGJIkn8+ne++9V7m5uYqPj1dmZqb+9Kc/afPmzRH1bNu2Tf3791dSUpJSUlI0aNAg7dq1K6LMmjVrdOGFFyomJkaNGjXSxIkT92rP3LlzlZOTo5iYGOXm5mr+/PlH7s0DAAAAOOlZmpBNmDBB06dP17Rp07Ru3TpNmDBBEydO1NSpUyVJJSUl+vzzzzVy5Eh9/vnn+ve//63169fryiuvjKinf//+Wrt2rfLz8/X2229r6dKluuWWW8LzvV6vunbtqqysLK1cuVKPPfaYRo8erZkzZ4bLLFu2TP369dOgQYO0atUq9ezZUz179tRXX311dFYGAAAAgJOO08qFL1u2TFdddZV69OghSWrSpIleeeUVffbZZ5Kk5ORk5efnR7xm2rRpOvfcc7Vx40Y1btxY69at04IFC7R8+XK1a9dOkjR16lRdfvnlevzxx5WZmalZs2apoqJCL7zwgtxut1q1aqXVq1dr0qRJ4cRtypQp6t69u0aMGCFJGjt2rPLz8zVt2jTNmDHjaK0SAAAAACcRSxOy9u3ba+bMmfr222912mmn6YsvvtBHH32kSZMm7fc1O3bskM1mU0pKiiSpoKBAKSkp4WRMkrp06SK73a5PP/1UvXr1UkFBgTp27Ci32x0u061bN02YMEHbt29XamqqCgoKNHz48IhldevWTfPmzdtnO8rLy1VeXh5+7vV6JZnDLH0+36GuiloXasOx0BYcP4gbRIO4QbSIHUSDuEE0jnbcHMpyLE3I7rvvPnm9XuXk5MjhcMjv92vcuHHq37//PsuXlZXp3nvvVb9+/ZSUlCRJKiwsVP369SPKOZ1OpaWlqbCwMFwmOzs7okx6enp4XmpqqgoLC8PTqpcJ1bGn8ePHa8yYMXtNX7RokeLi4mrw7o+OPXsYgZogbhAN4gbRInYQDeIG0ThacVNSUlLjspYmZHPmzNGsWbM0e/bs8DDCYcOGKTMzUwMGDIgo6/P5dO2118owDE2fPt2iFle5//77I3rUvF6vGjVqpK5du4aTRSv5fD7l5+fr0ksvlcvlsro5OE4QN4gGcYNoETuIBnGDaBztuAmNnqsJSxOyESNG6L777lPfvn0lSbm5ufrpp580fvz4iIQslIz99NNPWrJkSUTCk5GRoaKiooh6KysrtW3bNmVkZITLbNmyJaJM6PnByoTm78nj8cjj8ew13eVyHVMbh2OtPTg+EDeIBnGDaBE7iAZxg2gcrbg5lGVYepXFkpIS2e2RTXA4HAoEAuHnoWTsu+++07vvvqs6depElM/Ly1NxcbFWrlwZnrZkyRIFAgGdd9554TJLly6NGMuZn5+vFi1aKDU1NVxm8eLFEXXn5+crLy+vdt4sAAAAAOzB0oTsiiuu0Lhx4/TOO+/oxx9/1BtvvKFJkyapV69eksxk7JprrtGKFSs0a9Ys+f1+FRYWqrCwUBUVFZKkli1bqnv37rr55pv12Wef6eOPP9aQIUPUt29fZWZmSpL++Mc/yu12a9CgQVq7dq3+9a9/acqUKRFDDu+66y4tWLBATzzxhL755huNHj1aK1as0JAhQ47+igEAAABwUrB0yOLUqVM1cuRI3XHHHSoqKlJmZqZuvfVWjRo1SpK0adMmvfnmm5Kks846K+K17733ni666CJJ0qxZszRkyBB17txZdrtdvXv31lNPPRUum5ycrEWLFmnw4MFq27at6tatq1GjRkX8V1n79u01e/ZsPfjgg3rggQfUvHlzzZs3T2ecccaRXQkAAAAATlqWJmSJiYmaPHmyJk+evM/5TZo0kWEYB60nLS1Ns2fPPmCZ1q1b68MPPzxgmT59+qhPnz4HXR4AAAAA1AZLhywCAAAAwMmMhAwAAAAALEJCBgAAAAAWISEDAAAAAIuQkAEAAACARUjIAAAAAMAill72HsBxwlcmeTdJDpeU1FCycy7HMjs2ST9/Km1ZKyVlShm5Uv3TJU+C1S07vhmGtO0HyRUnJTWwujUHZhjS7q2S3SF5Es3vZW0L+KXKMqmyPHhfZk6zO83l2Z2S3SU5gvdOj9keHL9CfzNks1nbjhNBZYVUul0q3SaVeSVXjORJMm8xSUfmO3usKt8luWLZPhwECdmJ6PfvZSv+RYmlv0g7f5US65sbgwPxV0rlXql8p2SzS84Y8zXOWHOHW9sqdktF30hbvpJ2FppfVIcruIN3mc9Dj22O4HOHeRBgC97b7VXzIu7t5vzYNCm+rnmgUBO+Uqnkd/NAZ/dWafdvUknwPjTNHS/VPU2qd5p5X6eZuaHZk2FIZcXme9tZaNYdkyzFppj3McmSO+HI7PgqdkvezeZnGJdmHmAeaDmVFdLuImnXFtmKN6vJb+/KvmS55P1FKv5Z2vGztGtLVXlnrFS3mfn+654m1W1u3sfVMZddscu8L99V9dhfUe3zdZuPHcHHdkfwQMCQDAXvg88DfslXEqw3ePMF7yvLpORG5mdQ9zSpTtN9fxaSGdfbf5KKf5KKN5rLjq8vJdSX4uuZ99U/D8Mwl7t7qxkToZvDJaU1NZflSdz/OvWVSb99IxV+GYzxXyVXvBk/7nhzWe54M4lyxwd31InV7oM3IyAVrpF+/sxMwn7+zEyM92KT0rKl9DPMW8YZUtIp5meyvxgwDGlXkbR9g7RtQ9W9ETBfE1fH/A7FpVU9d8aYB+j+iqqDdX+FbOUlyij+QrbNGVJqY3N97mvnaxjm92Hreum3b811tPNXs+7EBlJiRuT9/uqpDWU7pE0rpV9WmOt10wrzAEqSGp4rteolteppJr0HUllufsbbNpgx4yutdh+8yTBjNS1bSm0ipWabB2UH4/eZ9W5dL239Vtr6XdV9ubeqnDM2Mm5iksz1l9pESskKLrOJOS10MqXMa9b12/rg5xG87f4tmHxVHtr6tDmklMZS2qnBW3bV46RM871UlkWul8pS2cp2q573K9k2pkqeOHOb4PSY3zVnjBSTIrnjatYGv89s/64tZmzv2lLtcVHVtJhkqeE5UqNzzVtyo+i2xYZhfhcqdptxENpHhfdFTvNm+KUdv5jb0uKNVdvV4o3m9yE2RUrIkBLTg3GfXhX/ht9cVxUlwW1fSVV8uePM7VhoGxZf19yGVH8vleVSabG5Pyrdbj4Or6PgbWe1x5K5Pa1/ulS/ZdUt6ZSqev2V5r5xV1Fw3/GbuX0Mbf/D2+xdVesmJnnv7UlsmhSbar7HyvJgTFQ7AVBZZh6PhNZjtRMBNtl0yvaVsq3xSvKb+zF/tVtsqpTeynwfcWk1/0wry839RehWsSvycXhftMfj8p3m+i0JJmEVuw68HGdM1fY+9PklZpj3CelmPCTUM2MsYr9arR2GITndwX1q6DvjMZ/b7AruUKsS7RC7M3gsklJ170k6/BOthmHuYwu/lH5dY+67fl0j7dxstieuTnC/W69a3NYz215d9fj1lZjbqtDxafXHNvsex1XV31Oy1OLy4+pEpc2oyT8v46C8Xq+Sk5O1Y8cOJSXVYEd7JC1+WPrwichprjhzAxWbagaqrzS4kQkGtq9k//XZneYO3xVTbaNaZ+8Nqzu+6qDb7gyeOQ2ePS353Tyjv+Ur837bDwpvLI40T1JwQ1DX3ADEppnvt3Rb8EB7u3lfWRpF5TbzIKTuaebOMZSA7SyU/OUHeakjmJjFmxsyIyAzGQlU3Wz24M45Y++D1ZgUM/HavsFcn6GD6urJk2RunGNTq3Z+sSnmZx46OCkrrtlbdcWZBzwB36GvpqPCZh5Y1W1mHgSWbpe2/2gmYiVbD/5yZ6y5cwgEzPKVZQcuH1/fTAbrnGomaXaHVPiVGeO/rTcPMg77LTn2rsfmMBOujNZmMlP4lbSr8MD1ODxV39nYVHPdbNtgHuAdCTaHGaNJmebNFVuVTFRPJGpal81e7WYL3juk5FPM5LhOM/OWFnwcX9f8TpX8biaw3s2R979+YX5Ge26DHB7zYK769MZ5UqurpdOvMg+Wtv1QlchtWmkedPgrDn0dxaaZSVIoWakInsAo3xV5ALbf7aTtAPP2w+E2t1cVJeZBUk3Zg8mR3W4ejAd8ZpuP1jbcGVttn1MnuM9JNddR9YSi5Pfo2pSQEZmcle0IJi973Mp2VEs2Sszl18b3vDY5POZ+TobZ5gPt2w+FJ5jkl2yVSrbpqH32tSHpFDM5S29lnrCKSd7HdiF4O9Tt04HY7MFkJzGY6Hlr7/OobTZ78FgpbY+TY9WPOZLN44U9T1Tu3mrui7Z8ZX5HjhXDv9lrtIPP59P8+fN1+eWXy+U68r2Uh5IbkJDVkmMqIfvoSRmfv6wKb5Hc/t2yGYGav9YZYyYC0RxgHKr4+lL66eYZXMNv9oaEDvj3fGwEzDO2Ab95Hyof8Fc9NvzmwbQRfG3ptkM/y2t3BRO3ulJcMIGLrxd8Xsfc2Gxdbx5c/rb+4MlMbKq5s3fHmWd2yorNM5RHOqlxJ5g7gJoux+6U4uvLiK+rwlKH6p92jhypWebBW0ojKbmxuaEO+M0zYFu/rXYLrovyneZyQ70+4V6g4JAqf6UZV6GDOX9F8DOulGQzjy9lC54ds1UdeLvj9+hdCt7sTjPZ+v07sw0H/SyCB8Apjc1lhhLS3b/tfyfpcAfjIHgQ6CuVfv++ZglebGqwt6q1lJoVPMu9v7Or3uCZv+BJkurfv5gUqdF5wQPG86RT2pjvv7rdW4O9cdVOeuwqMr8DB/wu26TkhuZ6Scs2e26cnuCONnjConR71Y63stzcRjiDZ2SDjwMOt4q3FinVUSrbrsLgyYX9LdJuJsx1W5g9zckNzZMiO38NnszYbN7v2nLgeg7EnWieEDnYdiy1idlT0vAcqWE7KT3XfJ/r3pS++rf08yeR6yomad8HHLFp5sGeO95MPl1xwVEGwcdGwPzebP/RTIRrEj8hrviqXuhQz3zd08x1aHNUnVSrfivbYR5obv+x6rbj5723hwkZUr0W5q3uaVK9HDPJdcaaceCMOfBQxOrb6TJv5MmhbT9UPa7YGVyFDnN9uIIn+FxxMuwueXcUKynOLVvAF9nTEW1PXUL9aj0NwftQj3hCfTO2fv7MvBWuOfRl7HO59gPHqzPW3JamNDaTvpRG5r4vId38vHYVmonlzl+DSeavZs+T3WmuL3dccDsYV7UOK3YHe6mCozhC63nvxlX1JMSmmtuyhIw9emWC90ZAKlpn3n4L3m/9bh8nhvbo8YirG9z2J+yxrU4wt6NlO4InQbdFbl/KioMnfWOq4q36vYyq/USgMhxvgcoK/b59h+rUbyC7KyY44sJTNQJj1xZzW1i8MbrP0x18L55E8315Equm7TnSIfQ4NrXqpFdsqrnt3rPXqfpopNB3db89lkXmd2/P/Wnosc0e3M75gqMVQvvV8r17xaSqnqfKiqpjkbLig598PBR2l1Q/R8o4U2rQ2tz/1c8x27f7t6p4DfWu7t5a9f0Lt7laz54rztzuhkaQxCRV9SyGRiKFTpaE31Pwcb9/7dW7TkJ2EjimEjJVC7rLussVKKs6w1eyzQxWZ0xkYMckR/4WIfT7AV+1oQOhruOS36v1LlXbwPpKqm0wKyMfexLMg9P6p1edqUqof2RXQujLuucQxJLfzR3bvnr5PIk1H7oS+h3H1m/NJM3vqxpqkphhPt7XUFHDMNdnaMPh21119l82RfQGBHzBDXRhtQPWQvNMXul28+x6araU1iR4HzyojkurGuoQcZZ3m7lcd0LkQUtwx3FYGyvDsO63B6HekK3fmQna9h/NHWJoqFZK1oGHiJXvqhp6Y3dWJWD7G1ZatsNMzLb9YN7//j/zs0pvZR7UZ+San02066Oy3Pyu+culxMzoh5IYhpnwlWyL7BGOSTLjJDWr5kN6DyAibuw2c11WP/tcsdvsyarbwryvyTIDfrPdhj+y1zh08/vMoV+//6/a7Xsz8QifvbeZMZ6UaZ4lT8o0v5/1WphJ2MG2QTt+kb7+j5mcbVphTnN4zAONU9qZSdwpbcx1eSifdWgI7fYN5vfZGVPtYGuPg6+4urXzm01/pZnsbv/RXF7d08wD9CMttB0KDUPcwwG3OaHXhrbb4YP54M2dENzmplcN84qrc2jrq6JE+nV1cEjwcrPeUOKy582TtPeJIVdc1eiQ0EiH6icPQwebsalHfvtYURIcSvibuazQiAhP8uHFUGWF+f3aXWTGY0L94Hq27jdBNd5Xle0wk8rQiaota81jldD2oPq2IekU8wSsJ/Hk+r2Tr6wqmSnZWm2kz6+Rj8t2VCX08XWDx07Bk9Xx9cyhrfVyzBN2x6hjOSHjN2QnutAY25hk88C0psJnZeIPXvZYFd4hpZpnmI9E/QnBMdBNLji017lizVuNLh6QG337PMEeq5RG0dVxqMuzis1W1bOZlXforw+tp7RTa1Y+Jtk8ED+lzaEvqyacHjOuDpfNVvW7otSsw6+vJhzOqgMdtYu+Hrvj4OugXgupeZfIab5SM9lxx5kH6IdzcJDcUMobbN6KfzaT2notD/+Aw5MYHHZ6xuHVcygczmCPd+Ojt0ypajt0OK/1JBza/utQuOOkrPbm7XDZbFW/Z7aCO05yH4HP2Ok2R7Po9Nqt92iISZYan2/esG+uGMkV/FkELENCBgA4cbhizSEytS2l0dE5sQEAOOlw7WoAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEVIyAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEWcVjfgRGEYhiTJ6/Va3BKTz+dTSUmJvF6vXC6X1c3BcYK4QTSIG0SL2EE0iBtE42jHTSgnCOUIB0JCVkt27twpSWrUqJHFLQEAAABwLNi5c6eSk5MPWMZm1CRtw0EFAgFt3rxZiYmJstlsVjdHXq9XjRo10s8//6ykpCSrm4PjBHGDaBA3iBaxg2gQN4jG0Y4bwzC0c+dOZWZmym4/8K/E6CGrJXa7XQ0bNrS6GXtJSkpiY4VDRtwgGsQNokXsIBrEDaJxNOPmYD1jIVzUAwAAAAAsQkIGAAAAABYhITtBeTwePfTQQ/J4PFY3BccR4gbRIG4QLWIH0SBuEI1jOW64qAcAAAAAWIQeMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISE7AT399NNq0qSJYmJidN555+mzzz6zukk4howfP17nnHOOEhMTVb9+ffXs2VPr16+PKFNWVqbBgwerTp06SkhIUO/evbVlyxaLWoxj0aOPPiqbzaZhw4aFpxE32J9Nmzbp//2//6c6deooNjZWubm5WrFiRXi+YRgaNWqUGjRooNjYWHXp0kXfffedhS2G1fx+v0aOHKns7GzFxsaqadOmGjt2rKpfi464gSQtXbpUV1xxhTIzM2Wz2TRv3ryI+TWJk23btql///5KSkpSSkqKBg0apF27dh2190BCdoL517/+peHDh+uhhx7S559/rjPPPFPdunVTUVGR1U3DMeKDDz7Q4MGD9cknnyg/P18+n09du3bV7t27w2XuvvtuvfXWW5o7d64++OADbd68WVdffbWFrcaxZPny5fr73/+u1q1bR0wnbrAv27dv1wUXXCCXy6X//ve/+vrrr/XEE08oNTU1XGbixIl66qmnNGPGDH366aeKj49Xt27dVFZWZmHLYaUJEyZo+vTpmjZtmtatW6cJEyZo4sSJmjp1argMcQNJ2r17t84880w9/fTT+5xfkzjp37+/1q5dq/z8fL399ttaunSpbrnllqP1FiQDJ5Rzzz3XGDx4cPi53+83MjMzjfHjx1vYKhzLioqKDEnGBx98YBiGYRQXFxsul8uYO3duuMy6desMSUZBQYFVzcQxYufOnUbz5s2N/Px8o1OnTsZdd91lGAZxg/279957jQ4dOux3fiAQMDIyMozHHnssPK24uNjweDzGK6+8cjSaiGNQjx49jIEDB0ZMu/rqq43+/fsbhkHcYN8kGW+88Ub4eU3i5OuvvzYkGcuXLw+X+e9//2vYbDZj06ZNR6Xd9JCdQCoqKrRy5Up16dIlPM1ut6tLly4qKCiwsGU4lu3YsUOSlJaWJklauXKlfD5fRBzl5OSocePGxBE0ePBg9ejRIyI+JOIG+/fmm2+qXbt26tOnj+rXr6+zzz5bzz77bHj+hg0bVFhYGBE7ycnJOu+884idk1j79u21ePFiffvtt5KkL774Qh999JEuu+wyScQNaqYmcVJQUKCUlBS1a9cuXKZLly6y2+369NNPj0o7nUdlKTgqtm7dKr/fr/T09Ijp6enp+uabbyxqFY5lgUBAw4YN0wUXXKAzzjhDklRYWCi3262UlJSIsunp6SosLLSglThWvPrqq/r888+1fPnyveYRN9ifH374QdOnT9fw4cP1wAMPaPny5Ro6dKjcbrcGDBgQjo997buInZPXfffdJ6/Xq5ycHDkcDvn9fo0bN079+/eXJOIGNVKTOCksLFT9+vUj5judTqWlpR21WCIhA05igwcP1ldffaWPPvrI6qbgGPfzzz/rrrvuUn5+vmJiYqxuDo4jgUBA7dq109/+9jdJ0tlnn62vvvpKM2bM0IABAyxuHY5Vc+bM0axZszR79my1atVKq1ev1rBhw5SZmUnc4ITDkMUTSN26deVwOPa6qtmWLVuUkZFhUatwrBoyZIjefvttvffee2rYsGF4ekZGhioqKlRcXBxRnjg6ua1cuVJFRUVq06aNnE6nnE6nPvjgAz311FNyOp1KT08nbrBPDRo00Omnnx4xrWXLltq4caMkheODfReqGzFihO677z717dtXubm5uv7663X33Xdr/Pjxkogb1ExN4iQjI2Ovi99VVlZq27ZtRy2WSMhOIG63W23bttXixYvD0wKBgBYvXqy8vDwLW4ZjiWEYGjJkiN544w0tWbJE2dnZEfPbtm0rl8sVEUfr16/Xxo0biaOTWOfOnfXll19q9erV4Vu7du3Uv3//8GPiBvtywQUX7PXXGt9++62ysrIkSdnZ2crIyIiIHa/Xq08//ZTYOYmVlJTIbo88THU4HAoEApKIG9RMTeIkLy9PxcXFWrlyZbjMkiVLFAgEdN555x2dhh6VS4fgqHn11VcNj8djvPTSS8bXX39t3HLLLUZKSopRWFhoddNwjLj99tuN5ORk4/333zd+/fXX8K2kpCRc5rbbbjMaN25sLFmyxFixYoWRl5dn5OXlWdhqHIuqX2XRMIgb7Ntnn31mOJ1OY9y4ccZ3331nzJo1y4iLizP++c9/hss8+uijRkpKivGf//zHWLNmjXHVVVcZ2dnZRmlpqYUth5UGDBhgnHLKKcbbb79tbNiwwfj3v/9t1K1b17jnnnvCZYgbGIZ59d9Vq1YZq1atMiQZkyZNMlatWmX89NNPhmHULE66d+9unH322cann35qfPTRR0bz5s2Nfv36HbX3QEJ2Apo6darRuHFjw+12G+eee67xySefWN0kHEMk7fP24osvhsuUlpYad9xxh5GammrExcUZvXr1Mn799VfrGo1j0p4JGXGD/XnrrbeMM844w/B4PEZOTo4xc+bMiPmBQMAYOXKkkZ6ebng8HqNz587G+vXrLWotjgVer9e46667jMaNGxsxMTHGqaeeavz1r381ysvLw2WIGxiGYbz33nv7PK4ZMGCAYRg1i5Pff//d6Nevn5GQkGAkJSUZN954o7Fz586j9h5shlHtL88BAAAAAEcNvyEDAAAAAIuQkAEAAACARUjIAAAAAMAiJGQAAAAAYBESMgAAAACwCAkZAAAAAFiEhAwAAAAALEJCBgAAAAAWISEDAMACNptN8+bNs7oZAACLkZABAE46N9xwg2w221637t27W900AMBJxml1AwAAsEL37t314osvRkzzeDwWtQYAcLKihwwAcFLyeDzKyMiIuKWmpkoyhxNOnz5dl112mWJjY3Xqqafqtddei3j9l19+qUsuuUSxsbGqU6eObrnlFu3atSuizAsvvKBWrVrJ4/GoQYMGGjJkSMT8rVu3qlevXoqLi1Pz5s315ptvhudt375d/fv3V7169RQbG6vmzZvvlUACAI5/JGQAAOzDyJEj1bt3b33xxRfq37+/+vbtq3Xr1kmSdu/erW7duik1NVXLly/X3Llz9e6770YkXNOnT9fgwYN1yy236Msvv9Sbb76pZs2aRSxjzJgxuvbaa7VmzRpdfvnl6t+/v7Zt2xZe/tdff63//ve/WrdunaZPn666desevRUAADgqbIZhGFY3AgCAo+mGG27QP//5T8XExERMf+CBB/TAAw/IZrPptttu0/Tp08Pzzj//fLVp00bPPPOMnn32Wd177736+eefFR8fL0maP3++rrjiCm3evFnp6ek65ZRTdOONN+qRRx7ZZxtsNpsefPBBjR07VpKZ5CUkJOi///2vunfvriuvvFJ169bVCy+8cITWAgDgWMBvyAAAJ6WLL744IuGSpLS0tPDjvLy8iHl5eXlavXq1JGndunU688wzw8mYJF1wwQUKBAJav369bDabNm/erM6dOx+wDa1btw4/jo+PV1JSkoqKiiRJt99+u3r37q3PP/9cXbt2Vc+ePdW+ffuo3isA4NhFQgYAOCnFx8fvNYSwtsTGxtaonMvlinhus9kUCAQkSZdddpl++uknzZ8/X/n5+ercubMGDx6sxx9/vNbbCwCwDr8hAwBgHz755JO9nrds2VKS1LJlS33xxRfavXt3eP7HH38su92uFi1aKDExUU2aNNHixYsPqw316tXTgAED9M9//lOTJ0/WzJkzD6s+AMCxhx4yAMBJqby8XIWFhRHTnE5n+MIZc+fOVbt27dShQwfNmjVLn332mZ5//nlJUv/+/fXQQw9pwIABGj16tH777Tfdeeeduv7665Weni5JGj16tG677TbVr19fl112mXbu3KmPP/5Yd955Z43aN2rUKLVt21atWrVSeXm53n777XBCCAA4cZCQAQBOSgsWLFCDBg0iprVo0ULffPONJPMKiK+++qruuOMONWjQQK+88opOP/10SVJcXJwWLlyou+66S+ecc47i4uLUu3dvTZo0KVzXgAEDVFZWpieffFJ/+ctfVLduXV1zzTU1bp/b7db999+vH3/8UbGxsbrwwgv16quv1sI7BwAcS7jKIgAAe7DZbHrjjTfUs2dPq5sCADjB8RsyAAAAALAICRkAAAAAWITfkAEAsAdG8wMAjhZ6yAAAAADAIiRkAAAAAGAREjIAAAAAsAgJGQAAAABYhIQMAAAAACxCQgYAAAAAFiEhAwAAAACLkJABAAAAgEX+P/OSYPSgunIgAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}